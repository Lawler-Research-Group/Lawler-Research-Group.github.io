<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://lawlergroup.lassp.cornell.edu/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lawlergroup.lassp.cornell.edu/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-16T22:12:00+00:00</updated><id>https://lawlergroup.lassp.cornell.edu/feed.xml</id><title type="html">Lawler Research Group</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Grover dynamics for speeding up optimization</title><link href="https://lawlergroup.lassp.cornell.edu/blog/2025/Grover's-algorithm-Blog-Post/" rel="alternate" type="text/html" title="Grover dynamics for speeding up optimization"/><published>2025-01-22T00:00:00+00:00</published><updated>2025-01-22T00:00:00+00:00</updated><id>https://lawlergroup.lassp.cornell.edu/blog/2025/Grover&apos;s-algorithm-Blog-Post</id><content type="html" xml:base="https://lawlergroup.lassp.cornell.edu/blog/2025/Grover&apos;s-algorithm-Blog-Post/"><![CDATA[<h1 id="1-getting-started">1. Getting started</h1> <p>Optimization problems are ubiquitous in both classical and quantum computing. In quantum mechanics, optimization arises in the <strong><em>search</em></strong> for a ground state of a Hamiltonian, the energy operator, which captures the low-temperature physics of the system. This search for minimum-energy states extends far beyond quantum systems: we encounter similar optimization challenges when minimizing cost functions in many different scenarios, from finding the shortest route in the Traveling Salesman Problem (TSP) to optimizing supply chain networks. This raises two compelling questions: What is the relationship between <strong><em>Grover’s search</em></strong> and <strong><em>Hamiltonian evolution</em></strong>, and can we leverage this connection to achieve quadratic speedup in finding optimal states?</p> <p>One way a quantum computer can outperform a classical computer is its ability to solve problems at a scale beyond brute-force classical simulations. In particular, Grover’s algorithm is a perfect quantum search algorithm with its superiority in searching through an unstructured database compared to classical computations. Having delved deeper into the algorithm under the hood and read this insightful paper by Stephen Fenner (Department of Computer Science and Engineering University of South Carolina), I was able to analogize how Grover’s algorithm searched through an unstructured database to how a simple Hamiltonian progressively finds a solution (Fenner, 2000).</p> <h1 id="2-grovers-algorithm">2. Grover’s algorithm</h1> <h2 id="21-high-level-overview-of-grovers-algorithm">2.1 High-level overview of Grover’s algorithm</h2> <p><strong><em>Let’s first break down Grover’s algorithm</em></strong>. Grover’s algorithm is a well-known search algorithm with a quadratic speed-up. The algorithm has two main parts: initialization and Grover’s operator. While it is typical to compartmentalize the algorithm into three parts (i.e. initialization, Grover’s oracle, and diffusion operator), I will combine the last two parts into one for a more intuitive visualization. The initialization step creates an equal superposition of all possible states, effectively preparing the quantum system to explore the entire search space simultaneously. The Grover operator then iteratively applies two key actions: it marks the target state by flipping its phase and amplifies its amplitude through interference, making it more likely to be measured. Section 2.2 handles a mathematical low-level breakdown of Grover’s algorithm, while Section 2.3 offers a more conceptual low-level deep dive into Grover’s algorithm using operator notation.</p> <h2 id="22-low-level-mathematical-overview-of-grovers-algorithm">2.2 Low-level (mathematical) overview of Grover’s algorithm</h2> <p>Initialization is an inherent simple step. We initialize a superposition state that uniformly covers all possible values |x\rangle within the search space of \(N=2^n\) total elements by applying one layer of Hadamard gates \(H^\{\otimes n}\) to each n number of qubits (Eq. 1). This produces an equal amplitude superposition over computational basis states labeled by bit strings, i.e. [ H^{\otimes n}|0\rangle^{\otimes n}=\frac{1}{\sqrt{2^n}} \sum{x=0}^{2^n-1} |x\rangle \tag{1} ] where x is a bit string. Initialization aims to prepare a state where all solutions are equally likely, allowing the algorithm to leverage quantum interference to amplify the amplitude of the marked state through Grover’s operator, as mentioned in the earlier section. This step aims to prepare the states before applying Grover’s operator, which has a circuit depth of one, independent of the number of qubits n.</p> <p>The primary complexity in Grover’s search algorithm originates from Grover’s operator G. This operator comprises two structures: Grover’s oracle O and the diffusion operator D. Grover’s oracle is a unitary operator that recognizes and marks the target state/solutions to the search problem by applying a phase shift of -1 to the solution space. In Equation 2.1, |x\rangle is the state representing one of the possible states of the qubits, |q\rangle is the ancillary qubit (typically initialized to |0\rangle or |1\rangle), and \(f(x)\) is the oracle function that defines the search problem where we look for bit strings \(x\) that satisfy \(f(x)=1\). Grover’s oracle then applies a phase shift to this space, which we can implement using the ancilla qubit via [ O|x\rangle|q\rangle=|x\rangle|q\oplusf(x)\rangle \tag{2.1} ] Equation 2.1, which represents the oracle using an ancilla qubit, comes in handy when designing and coding a circuit that uses Grover’s algorithm (Bartschi &amp; Eidenbenz, 2020). The oracle function is very abstract, and it turns out that by leveraging the ancilla while designing the circuit, we can get a better idea of how computationally expensive applying an oracle to the states can be. For instance, Figure 1 depicts my implementation of Grover’s algorithm on a 5-qubit system with four iterations of Grover’s operator G (which will be covered in the upcoming few paragraphs).</p> <p align="center"> <img src="https://imgur.com/0ZHBQbt" width="75%"/> Figure 1 </p> <p>However, we do not have to use an ancilla qubit to execute Grover’s search at an abstract level since we simply need to apply the oracle function when doing mathematical computations. Hence, it is easier to mathematically represent the computation of an oracle function by defining the transformation shown in Equation 2.2 instead of trying to describe an ancilla qubit mathematically. By definition, \(f(x)=1\) for the marked state(s), and \(f(x)=0\) for all other states. [ |x\rangle=(-1)^{f(x)}|x\rangle \tag{2.2} ] This way, the oracle computation seems more intuitive as it shows that it is the oracle function f(x) that flips the phase of |x\rangle if x satisfies the search condition. To implement the oracle on a circuit, we can \cf0 achieve this with the ancilla by applying a multi-CNOT gate (i.e. multi-controlled Toffoli gate) where the ancilla qubit is the target qubit and the rest of the qubits are the controlled qubits. In the code snippet in Figure 1, this multi-CNOT gate is implemented using <strong>qc.mct(qr, anc[0], None, mode=’noancilla’)</strong>. Mathematically, the state after applying the oracle is as such in Equation 3. [ |\psi\rangle=O\frac{1}{\sqrt{2^n}}\sum{x=0}^{2^n-1}|x\rangle=\frac{1}{\sqrt{2^n}}\sum{x=0}^{2^n-1}(-1)^{f(x)}|x\rangle \tag{3} ] After applying the oracle, we carry out three steps that amount to the diffusion operator. First, we apply the Hadamard transform again to all qubits [ |\psi\rangle=\frac{1}{\sqrt{2^n}}\sum{x=0}{2^n-1}(-1)^{f(x)}H^{\otimes n}|x\rangle=\frac{1}{2^n}\sum{y=0}{2^n-1}\left(\sum{y=0}{2^n-1}(-1)^{f(x)+x\cdoty}\right)|y\rangle. \tag{4} ] The Hadamard gates are applied to transform the uniform superposition basis back into \cf0 the computational basis. Subsequently, we apply the phase shift defined in Equation (5), which performs a conditional phase shift on the computer with every computational basis state except |0\rangle for receiving a phase shift -1. [ P: |x\rangle=-(-1)^{\delta*{x,0}}|x\rangle \tag{5} ] \(\delta*{x,0}\) is the Kronecker delta function, which is defined as [ \delta_{x,0}= \begin{cases} 1 &amp; \text{if } x=0 <br/> 0 &amp; \text{if} x \neq 0 \end{cases} ] where x is the computational basis state (e.g. |x\rangle). This conditional phase shift will apply a phase shift of -1 to every computational basis state except |0\rangle.</p> <p>Thus, if we apply P to |\psi\rangle, we get Equation (6). [ |\psi\rangle=\frac{1}{2^n}\left(\sum{x=0}{2^n-1} (-1)^{f(x)}\right)|0\rangle+\sum{y=0}{2^n-1}\left(\sum{x=0}{2^n-1}(-1)^{f(x)}(-1)^{x\cdoty}\right)|y\rangle \tag{6} ] Lastly, we apply the Hadamard transform again, ending in a final state |\psi\rangle shown in Equation (7). [ |\psi_{final}\rangle=H^{\otimesn}\left(\frac{1}{2^n}\left(\sum{x=0}{2^n-1} (-1)^{f(x)}\right)|0\rangle+\sum{y=0}{2^n-1}\left(\sum{x=0}{2^n-1}(-1)^{f(x)}(-1)^{x\cdoty}\right)|y\rangle\right) \tag{7} ]</p> <h2 id="23-low-level-operator-notation-overview-of-grovers-algorithm">2.3 Low-level (operator notation) overview of Grover’s algorithm</h2> <p>Now, as you can see, it gets extremely mathy as we go through the Grover’s Operator since the oracle and the diffusion operator involve multiple operators. However, we can simplify the math representation using operator notation, which will lead us to how we can conceptually understand Grover’s Operator and, by extension, Grover’s search algorithm. First, we can represent Equation (5) in a slightly different way by replacing the exponential form with a simpler linear expression as shown in Equation 8.1. [ P: |x\rangle=\left(2\delta_{x,0}-1\right)|x\rangle \tag{8.1} ] Then, Equation (8.1) can be expressed as operator form using a projection operator |0\rangle\langle0| onto the state |0\rangle as shown in Equation (8.2). [ P: |x\rangle=(2|0\rangle\langle0|-I)|x\rangle \tag{8.2} ] 2|0\rangle\langle0|x\rangle is defined as [ |0\rangle\langle0|x\rangle= begin{cases} |0\rangle &amp; \text{if } x=0 <br/> 0 &amp; \text{if } x \neq 0 \end{cases} ] Subsequently, |x\rangle gets a phase flip (multiplied by -1). This shows that Equation (8.1) and (8.2) have the same effect.</p> <p>The Hadamard transform that is applied on Equation (8.2) results in Equation (8.3) by conjugating it with \(H^\{\otimesn}\). Two H operators are required as shown below because the Hadamard transform is applied before and after the operator 2|0\rangle\langle0|-I to change its reference basis also known as “conjugating the operator”. [ H^{\otimesn}(2|0\rangle\langle0|-I)H^{\otimesn}=2|\psi\rangle\langle\psi|-I\equivD \tag{8.3} ] Hence, it turns out that everything that comes after the oracle (i.e. diffusion operator) can be represented as Equation (8.3). Therefore, we can represent the entire Grover’s Operator G as shown in Equation (9), including the oracle operator. [ G=(2|\psi\rangle\langle\psi|-I)O \tag{9} ] Equation (9) enables us to acknowledge how Grover’s Operator, which is the algorithm’s core, is much simpler than it seems in Equation (7). Moreover, it helps us to visualize what G is doing to the initial state to find the target state.</p> <table> <tbody> <tr> <td>To show this, I am going to define the target state in terms of</td> <td>\alpha\rangle and</td> <td>\beta\rangle, which are basis vectors that represent non-solutions and solutions respectively. Hence, we can represent</td> <td>\psi\rangle=a</td> <td>\alpha\rangle+b</td> <td>\beta\rangle where <strong>a</strong> and <strong>b</strong> are arbitrary constants since</td> <td>\psi\rangle is a combination of an arbitrary proportion of solutions and non-solutions. To be more specific, I will define</td> <td>\alpha\rangle\equiv\frac{1}{\sqrt{N-M}}\sum{x}{}</td> <td>x\rangle and</td> <td>\beta\rangle\equiv\frac{1}{\sqrt{M}}\sum{x}{}</td> <td>x\rangle where N is the total number of elements and M is the total number of solutions. Having defined</td> <td>\psi\rangle, we can also graph this vector on</td> <td>\alpha\rangle and</td> <td>\beta\rangle axes as shown in Figure 2 below.</td> </tr> </tbody> </table> <p align="center"> <img src="https://imgur.com/G4k7KSz" width="75%"/> Figure 2 </p> <p>Figure 2 also shows us the effect of the oracle and the diffusion operator, essentially visualizing Grover’s operator. The oracle performs a reflection about the vector |\alpha\rangle, which portrays the oracle marking the target state by applying a phase shift of -1, also shown in Equation (10) and Figure 2. [ O(a|\alpha\rangle+b|\beta\rangle)=a|\alpha\rangle-b|\beta\rangle \tag{10} ] Then, the diffusion operator 2|\psi\rangle\langle\psi|-I also performs a reflection but on the vector |\psi\rangle. Hence, Grover’s Operator G is the product of those two reflections and is effectively a rotation towards the basis vector |\beta\rangle, representing the solution(s)/target state(s)! If we apply G multiple times, |\psi\rangle would rotate closer towards |\beta\rangle, which we refer to as Grover’s Iterations G^k|\psi\rangle as mentioned earlier. G^k\psi\rangle remains in the space spanned by |\alpha\rangle and |\beta\rangle for all \(k\in\mathbb{N}\). The optimal number of iterations that leads to the target state exists, so the more we iterate doesn’t necessarily mean the more likely we are going to get to the target state. The so-called Grover’s Iterations resemble the process of amplitude amplification as each iteration increases the chances of reaching the target state by rotating closer to |\beta\rangle. An alternative way to represent |\psi\rangle is also shown in Equation (11), where \(\theta\) is the rotation angle. [ |\psi\rangle=\cos(\frac{\theta}{2})|\alpha\rangle+\sin(\frac{\theta}{2})|\beta\rangle ] [ G|\psi\rangle=\cos(\frac{3\theta}{2})|\alpha\rangle+\sin(3\frac{\theta}{2})|\beta\rangle ] [ G^k|\psi\rangle=\cos(\frac{(2k+1)\theta}{2})|\alpha\rangle+\sin(\frac{(2k+1)\theta}{2})|\beta\rangle \tag{11} ] In a nutshell, Grover’s algorithm is an operator that finds the target state by a series of discrete rotations (made possible by the two successive reflections on |\beta\rangle and |\psi\rangle) towards |\beta\rangle, thereby increasing the amplitude of the target state.</p> <h1 id="3-hamiltonian">3. Hamiltonian</h1> <h2 id="31-fenner92s-hamiltonian">3.1 Fenner'92s Hamiltonian</h2> <p><strong><em>Having said this, how can we parallel Grover’s algorithm to a Hamiltonian?</em></strong> In his article called “An Intuitive Hamiltonian for Quantum Search”, Stephen Fenner describes a Hamiltonian for Grover’s algorithm based on Farhi &amp; Gutmann’s Hamiltonian (Fenner, 2000). I didn’t find Fenner’s paper “intuitive”, so I went through the mathematical steps to intuitively compare how similar this Hamiltonian was to the Grover’s algorithm.</p> <p>First, we derived the Hamiltonian from Fenner’s Hamiltonian as shown in Equation (12), where |w\rangle is the target state ranging from 0 to N (search space) and |\sigma\rangle is an arbitrary unit vector (initial/start state) in the Hilbert space. We also assume x=\langlew|\sigma\rangle\in\mathbb{C} where x=\langlew|\sigma\rangle and x^{<em>}=\langle\sigma|w\rangle. Fenner’s Hamiltonian is then [ H=2iE\left(x\cdot|w\rangle\langle\sigma|-x^{</em>}\cdot|\sigma\rangle\langlew|\right) \tag{12} ] If we let |w\rangle=A|\sigma\rangle+B|\sigma<em>\perp\rangle and \langlew|=A^{*}\langle\sigma</em>\perp|+B^{*}\langle\sigma<em>\perp| (and \langle\sigma|\sigma</em>\perp\rangle=0), we can evaluate the Hamiltonian further to give Equation (13). [ H=E[(0)I+I(A^{*}B-AB^{*})X+(A^{*}B+AB^{*})Y+(0)Z] \tag{13} ] This looks complicated, but we can think about it in a more intuitive way by mapping it to a spin in a magnetic field. To do so, let’s define another Hamiltonian H constructed using an identity operator I, a scalar factor E, and a Pauli matrix \vec{\sigma} scaled by a parameter \vec{r} vector (the axis of rotation in which the initial state rotates about to progress to the target state) shown in Equation (14), representing some sort of spin-system to resemble the rotational nature of Grover’s algorithm. This allows us to define the evolution of the initial state |\psi(t)\rangle over time using the time evolution operator, as shown in Equation (15), where \hat{n} is the axis of rotation where the initial state is continuously rotated to the final state.. [ H=E(I+\vec{r}\cdot\vector{\sigma}) \tag{14} ] [ |\psi(t)\rangle=e^{-itE|\vec{r}|\hat{n}\cdot\vec{\sigma}}|\sigma\rangle \tag{15} ]</p> <h1 id="4-grovers-algorithm-vs-hamiltonian">4. Grover’s algorithm vs Hamiltonian</h1> <p>So, how is this Hamiltonian similar to Grover’s search? H resembles Grover’s search in two main ways.</p> <table> <tbody> <tr> <td>First, both methods involve the initial state evolving to the target state through rotation. Grover’s algorithm rotates the initial state to the target state through a set of <strong><em>discrete</em></strong> rotations referred to as Grover’s iterations, while the Hamiltonian rotates the initial state about the axis of rotation \vec{r}, in which its direction depends on</td> <td>\sigma\rangle and</td> <td>w\rangle.</td> </tr> </tbody> </table> <p>Second, the time variable in the time evolution operator plays a role similar to the number of iterations in Grover’s algorithm. The initial state as a function of time is represented in Equation (15), but it can also be represented as |\psi(t)\rangle=\alpha(t)|w\rangle+\beta(t)|\sigma<em>\perp\rangle where \alpha(t)=\langlew|\psi(t)\rangle and \beta(t)=\langle\sigma</em>\perp|\psi(t)\rangle are periodic functions of t. There is a time t^{*} where |\alpha(t)|^2 is maximized, analogous to the optimal number of iterations in Grover’s search. The period of these oscillations will be proportional to \frac{1}{\sqrt{N}} where N is the size of the search space. If we were to find that t^{*}, we would have to find the time t when |\alpha(t)=\langlew|\psi(t)\rangle|^2=1. After evaluating this equation, we get Equation (16) below which can be graphed to visualize (Fig. 3) where \theta represents E|\vec{r}|t. [ \sin^2(\theta)+2|A||B|\sin(\theta)\cos(\theta)+|A|^2\cos(2\theta)=1 \tag{16} ]</p> <p align="center"> <img src="https://imgur.com/frUbqd6" width="75%"/> Figure 3 </p> <table> <tbody> <tr> <td>Hence, depending on $$</td> <td>A</td> <td>\(, we can see the t^{\*} values changing. Regardless, the point is that there exists a t^{\*} value that maximizes\)</td> <td>\alpha(t)</td> <td>^2$$, thereby reaching the target state.</td> </tr> </tbody> </table> <p>This is all to say Grover’s algorithm operates in a similar way, where the “landscape” of probabilities (analogous to the amplitude of states) is adjusted with each iteration to amplify the correct solution. The reflections and rotations in Grover’s algorithm are effectively “searching” the state space to maximize the probability of finding the target state, just as you are looking for the \theta that maximizes \(f(\theta)\).</p> <h1 id="5-final-remarks">5. Final remarks</h1> <p>The connection between Grover’s search and Hamiltonian dynamics is a bridge between quantum search and physical evolution. Understanding this relationship provides insights into quantum speedups in areas such as molecular dynamics, quantum machine learning, and even quantum error correction. As we continue to explore this deep connection, we may discover entirely new quantum algorithms that harness the natural evolution of quantum systems to solve computational problems more efficiently than before. Indeed, my next research direction leverages this Hamiltonian-Grover connection to develop a novel approach for solving the Traveling Salesman Problem, potentially offering a new quantum speedup for this fundamental optimization challenge.</p> <p>[1] Bartschi, A., &amp; Eidenbenz, S. (2020). Grover Mixers for QAOA: Shifting Complexity from Mixer Design to State Preparation. ArXiv (Cornell University).</p> <p>[2]Fenner, S. A. (2000). An intuitive Hamiltonian for quantum search. ArXiv (Cornell University).</p>]]></content><author><name></name></author><category term="quantum"/><category term="algorithms"/><category term="optimization"/><category term="hamiltonian"/><summary type="html"><![CDATA[by Kevin Lee]]></summary></entry><entry><title type="html">AI: Assistant First, Researcher Second</title><link href="https://lawlergroup.lassp.cornell.edu/blog/2025/AI-Assistant-First-Researcher-Second/" rel="alternate" type="text/html" title="AI: Assistant First, Researcher Second"/><published>2025-01-21T11:31:50+00:00</published><updated>2025-01-21T11:31:50+00:00</updated><id>https://lawlergroup.lassp.cornell.edu/blog/2025/AI:-Assistant-First-Researcher-Second</id><content type="html" xml:base="https://lawlergroup.lassp.cornell.edu/blog/2025/AI-Assistant-First-Researcher-Second/"><![CDATA[<p>Four months ago, I embarked on an ambitious experiment: I wanted to prompt-engineer ChatGPT-3 into constructing a variance matrix from data of multiple Ising models, ultimately allowing it to generate its own Ising model based on the collected data. This task, which seemed plausible given the growing capabilities of AI, quickly illuminated the limitations of large language models (LLMs) in high-level scientific reasoning and computation. Despite its fluency and broad knowledge base, GPT-3 struggled with rigorous, mathematically intensive tasks that required deep reasoning and autonomous problem-solving. This realization, coupled with discussions with my mentor, spurred an ongoing inquiry: To what extent can foundational LLMs (large dataset AI such as ChatGPT or Llama) be fine-tuned for scientific research, particularly in condensed matter theory?</p> <h3 id="the-challenge-of-fine-tuning-llms-for-science">The Challenge of Fine-Tuning LLMs for Science</h3> <p>While fine-tuning LLMs for domain-specific expertise is a promising approach, the availability of well-trained scientific models remains limited. The closest counterpart I found was <a href="https://arxiv.org/abs/2404.08001">Xiwu</a> [1], a recently developed L2 (fine-tuned from a foundational model) LLM for high-energy physics. However, the landscape for condensed matter physics was sparse, and much of our data—such as scanning tunneling microscopy (STM) images—was more efficiently stored as images rather than text. Given these constraints, my mentor and I decided to pivot towards evaluating multimodal LLMs (MLLMs), particularly image-based models, to determine their efficacy in scientific analysis.</p> <h3 id="the-role-of-multimodal-llms">The Role of Multimodal LLMs</h3> <p>I have experience running LLMs locally, primarily using Llama models, which have evolved significantly from Llama2 to the contemporary Llama3.3. Historically, deploying and fine-tuning these models locally has been a cumbersome process, requiring different installation methods and optimization strategies depending on the specific model. However, the release of <a href="https://ollama.com/">Ollama</a>—a streamlined platform for running LLMs and MLLMs—greatly simplified this workflow. In the past two months, Ollama has expanded to support a diverse range of MLLMs, while also providing tools for UI development, API integration, and parameter management.</p> <p>To evaluate the capabilities of these MLLMs, I ran five different models using raw STM data from various materials, such as graphene. The largest model I tested, Llama3.2 Vision (11 billion-parameter model), demonstrated an impressive ability to recognize the general context of the images, even making reasonable predictions about the material composition and possible defects. However, all other models exhibited severe hallucinations, misidentifying the STM images as blankets or cloths.</p> <p align="center"> <img src="https://i.imgur.com/Y8KMmkY.jpeg" width="500"/> </p> <p align="center"> <b>Fig. 1 An example of STM data sent to the LLMs, the material here being graphite [2].</b> </p> <h3 id="the-impact-of-prompt-engineering">The Impact of Prompt Engineering</h3> <p>In an attempt to first refine the models’ performance without adding new data, I implemented a simple yet effective intervention: prompt engineering. By adding a subtext to each prompt (essentially instructing the models to assume the role of a distinguished professor in condensed matter physics) I observed a dramatic improvement in their responses. Now, all five models correctly identified the STM technique and provided coherent analyses of material defects. However, this approach had a crucial limitation: it did not enhance the models’ ability to generate figures or perform rigorous quantitative analysis. Rather, it merely refocused their responses toward condensed matter physics without deepening their computational reasoning.</p> <p align="center"> <img src="https://i.imgur.com/qh0kYN3.png" width="80%"/> </p> <p align="center"> <img src="https://i.imgur.com/ABVdcdy.png" width="80%"/> </p> <p align="center"> <b>Figs. 2,3 The prompt engineered modification of Llava, a moderately large model, answering without hallucination.</b> </p> <h3 id="exploring-retrieval-augmented-generation">Exploring Retrieval-Augmented Generation</h3> <p>Recognizing that additional data might be required, I turned to Retrieval-Augmented Generation (RAG), a technique that embeds external data into the model’s vector space, allowing it to better match prompts with relevant information. However, even when embedding the papers that contained the images I uploaded, there was no improvement in recognition or scientific reasoning. In fact, without prompt engineering, the smaller models continued to hallucinate no matter the parameter of attention I put on the papers within the LLM.</p> <h3 id="gpt-4-the-standout-in-multimodal-reasoning">GPT-4: The Standout in Multimodal Reasoning</h3> <p>Despite my focus on open-source solutions, I have also been experimenting with GPT-4, which supports multimodal capabilities. Unlike the other models I tested, GPT-4 has shown remarkable proficiency in analyzing images, even providing annotated feedback on STM data. It remains the only model capable of returning modified images, highlighting possible defects for example, rather than just describing them in text. Furthermore, GPT-4 has demonstrated greater flexibility in fine-tuning, offering a glimpse into the potential of well-optimized multimodal AI for scientific research.</p> <p align="center"> <img src="https://i.imgur.com/FPioA8f.png" width="500"/> </p> <p align="center"> <b>Fig. 4 GPT-4's impressive ability to communicate ideas through modifying uploaded images, evidenced by the red circles indicating possible defects.</b> </p> <h3 id="the-future-of-ai-in-scientific-research">The Future of AI in Scientific Research</h3> <p>These findings reinforce a key takeaway: while AI excels as a personalized assistant—summarizing papers, structuring research plans, and even contextualizing complex scientific ideas—it still falls short as an autonomous scientific researcher. The ability to engage in high-level reasoning, generate rigorous mathematical models, and independently conduct research on a local level remains beyond the reach of current LLMs and MLLMs, even with fine-tuning and retrieval techniques.</p> <p>However, the rapid evolution of AI suggests that these gaps may narrow in the coming years. With better fine-tuning techniques, accessible multimodal embedding programs, and more robust open-source research models, AI could eventually transition from a highly capable assistant to a legitimate collaborator in scientific discovery. Until then, researchers must leverage AI for what it currently does best—enhancing efficiency, improving accessibility, and accelerating preliminary analysis—while acknowledging its limitations in deep scientific reasoning.</p> <p>In the meantime, my next steps involve further experiments with RAG implementations, exploring alternative fine-tuning methods, and closely monitoring advancements in MLLMs. Whether through open-source models or proprietary solutions like GPT-4, the potential of AI in condensed matter physics remains an exciting frontier, albeit one that still requires significant human expertise to navigate effectively.</p> <p>[1]Zhang, Zhengde, et al. “Xiwu: A Basis Flexible and Learnable LLM for High Energy Physics”, arXiv:2404.08001, arXiv, 8 Apr. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2404.08001.</p> <p>[2]“Capillary Force-Induced Superlattice Variation atop a Nanometer-Wide Graphene Flake and Its Moiré Origin Studied by STM.” Beilstein Journal of Nanotechnology, vol. 10, Jan. 2019, pp. 804–10. www.sciencedirect.com, https://doi.org/10.3762/bjnano.10.80.</p>]]></content><author><name></name></author><category term="neuralnets"/><category term="materials science"/><category term="RAG"/><summary type="html"><![CDATA[by Tristan Galler]]></summary></entry><entry><title type="html">The Relativistic Quantum Information FAQ</title><link href="https://lawlergroup.lassp.cornell.edu/blog/2022/The_Relativistic_Quantum_Information_FAQ/" rel="alternate" type="text/html" title="The Relativistic Quantum Information FAQ"/><published>2022-10-30T00:00:00+00:00</published><updated>2022-10-30T00:00:00+00:00</updated><id>https://lawlergroup.lassp.cornell.edu/blog/2022/The_Relativistic_Quantum_Information_FAQ</id><content type="html" xml:base="https://lawlergroup.lassp.cornell.edu/blog/2022/The_Relativistic_Quantum_Information_FAQ/"><![CDATA[<p>A recent visit to Johns Hopkins University and the University of Maryland has me realizing that Relativistic Quantum Information (RQI) is too young in its infancy for many to know what it’s all about. I received many questions during conversations with friends and colleagues about RQI. This FAQ is a collection of those questions and others that may prove helpful. Please feel free to add any questions to the comment section below, and I will do my best to answer them.</p> <h1 id="questions">Questions</h1> <ul> <li><a href="#question1">What is RQI?</a></li> <li><a href="#question2">What classifies as quantum information?</a></li> <li><a href="#question3">What is relativistic about RQI?</a></li> <li><a href="#question4">Why add relativity?</a></li> <li><a href="#question5">When did RQI become an area of study?</a></li> <li><a href="#question6">What is an Unruh-DeWitt detector?</a></li> <li><a href="#question7">What other methods are used to study RQI?</a></li> <li><a href="#question8">What are current RQI physicists studying?</a></li> <li><a href="#question9">What future insights will RQI give?</a></li> </ul> <h3 id="what-is-rqi--">What is RQI? <a name="question1"> </a></h3> <p>RQI is a field of physics that combines the formalism of quantum information with relativity theory to understand how entanglement is transferred, created, and propagated in a relativistic setting.</p> <h3 id="what-classifies-as-quantum-information-">What classifies as quantum information? <a name="question2"></a></h3> <p>Information is best thought of as the measurement of a state. The word <strong>measurement</strong> has an important role to play here as there are many different measures of individual states (e.g., temperature, velocity, spin, etc.). As a quick aside, a state is not necessarily quantum. It could be the state of a system or a state of being, all of which carry information. Nevertheless, this is how we describe information.</p> <p>There are two forms of information; classical and quantum, but a defining feature that separates the two is <strong>entanglement</strong>. Quantum information is specific in that the measurement it’s concerned with is entanglement. Of course, there are many measures of entanglement between quantum systems, but to be considered quantum information, it must contain entanglement. All other information classifies as classical. I recently wrote a <a href="https://lawlergroup.lassp.cornell.edu/blog/2022/Quantum_channel/">blog post</a> on evaluating different types of information. If you are interested in a deeper dive, please check out that post and the references therein.</p> <h3 id="what-is-relativistic-about-rqi-">What is relativistic about RQI? <a name="question3"></a></h3> <p>The answer to this question is a bit tricky. The theory of RQI aims to tackle what we understand about quantum information, but in various ways that include relativity. To accomplish this, we use the language that handles quantum mechanics and special relativity; quantum field theory (QFT). However, QFT is a vast field. Specifically, the systems that I study, topological edge states of strongly correlated electrons, don’t <em>necessarily</em> include relativistic effects. Yet the machinery is utilized to great success and is equipped to handle relativistic velocities of Dirac fermions. Read on to find more (relativistic) use cases.</p> <h3 id="why-add-relativity-">Why add relativity? <a name="question4"></a></h3> <p>Regardless of the motivation, this has repeatedly been the natural tendency of a physical theory. First, in the early 1900s, mechanics was revolutionized by Einstein, as well as others. Then later, in the 1930s and ’40s, quantum mechanics was transformed into QFT by Dirac, Feynman, Schwinger, and more. This year (2022) brought the recognition of quantum information (QI) theory by Charles Bennet and Peter Shor et al. through various prizes, such as the 2023 Breakthrough Prize. Subsequently, the formalization of the theory in the mid to late 1990s wasn’t the first attempt to understand QI but instead marked the birth of the field. One may surmise that the transformation to a theory of RQI was inevitable, and as such, we have gained many more insights.</p> <h3 id="when-did-rqi-become-an-area-of-study-">When did RQI become an area of study? <a name="question5"></a></h3> <p>Similar to QI, RQI has been around for some time before the theory was laid out. It’s hard to pin down when people began thinking about relativistic entanglement. But it was certainly no later than the work of <a href="https://arxiv.org/abs/quant-ph/0302179">Alsing and Millburn</a>(2003) and formalized by <a href="https://arxiv.org/abs/1106.0280">Eduardo Martin-Martinez</a>(2011). Shortly after, a boom in the field took place with the introduction of the <a href="https://arxiv.org/abs/1207.3123">amps paradox</a>(2012) as well as the work developed by <a href="https://arxiv.org/abs/1306.0533">Maldacena and Susskind</a>(2013) in their famous ER=EPR paper.</p> <p>Eduardo’s approach to formalizing the theory was to calculate quantum information values utilizing Unruh-DeWitt detectors as the machinery to transfer entanglement to quantum fields via interactions, encoding (decoding) QI onto (off of) quantum fields in a process known as entanglement harvesting. Subsequent years have led to progress in understanding the requirements to carry out this type of quantum channel, and <a href="https://arxiv.org/abs/2210.12552">recent work</a> by John Marohn, Michael Lawler, and myself have connected this idea to experiment.</p> <h3 id="what-is-an-unruh-dewitt-detector-">What is an Unruh-DeWitt detector? <a name="question6"></a></h3> <p>UDW detectors were proposed as a thought experiment in Bill Unruh’s <a href="https://journals.aps.org/prd/abstract/10.1103/PhysRevD.14.870">1976 paper</a> “Notes on Black-hole Evaporation”. In this paper, Unruh imagined a detector with two states whose reading determined if radiation was present. These detectors, as he imagined, would be accelerated and subsequently detect radiation not present in an inertial frame of reference. This radiation became known as Unruh radiation.</p> <p>Eventually, the machinery was found promising for entanglement harvesting, and today RQI physicists use UDWs to understand particle interactions in curved space-time. One <a href="https://arxiv.org/abs/1908.07523">recent result</a> was the impracticality of wireless quantum communication by Simidzija et al. who showed that to decode the information from a quantum field, the detector would need to exist on the entirety of the light-cone that stems from the encoding interaction. This unreasonable requirement is what prevents wireless quantum communication.</p> <h3 id="what-other-methods-are-used-to-study-rqi-">What other methods are used to study RQI? <a name="question7"></a></h3> <p>As with QI, RQI has many subfields that most may not realize are studies of RQI. Two of the more famous problems tackled with RQI are ER=EPR and Emergent spacetime. ER=EPR links two famous papers of Einstein and, although controversial, ER=EPR tells a tale of entangled black holes linked together by a wormhole, which has consequences for the entangled connections of spacetime around the horizons of the black holes.</p> <p>Recently I was introduced to studies on emergent spacetime. Take, for example, this 2016 <a href="https://arxiv.org/abs/1606.08444">paper</a> by ChunJun Cao, Sean Carroll, and Spyridon Michalakis as well as the associated <a href="https://www.preposterousuniverse.com/blog/2016/07/18/space-emerging-from-quantum-mechanics/">blog post</a>. Carroll explains the result of this work as follows,</p> <p>“<em>The claim, in its most dramatic-sounding form, is that gravity (spacetime curvature caused by energy/momentum) isn’t hard to obtain in quantum mechanics — it’s automatic! Or at least, the most natural thing to expect. If geometry is defined by entanglement and quantum information, then perturbing the state (e.g. by adding energy) naturally changes that geometry. And if the model matches onto an emergent field theory at large distances, the most natural relationship between energy and curvature is given by Einstein’s equation.</em>”</p> <p>As admitted by Carroll in the post, this work is controversial and speculative. However, these two areas, as well as others, are recent and exciting fields that utilize relativity and quantum information to shed light on the fundamental underpinnings of the universe.</p> <h3 id="what-are-current-rqi-physicists-studying-">What are current RQI physicists studying? <a name="question8"></a></h3> <p>The areas discussed in this FAQ are all topics modern RQI scientists are thinking about, but in case you skipped directly to this question, let’s do a quick review with resources.</p> <ol> <li>Unruh-DeWitt detectors (This is a bit biased as this is my field of study) 1a. UDW detectors used to understand wireless quantum communication; <a href="https://arxiv.org/abs/1908.07523">Simidzija et al.</a> 1b. UDW detectors to understand interactions in curved spacetime; <a href="https://arxiv.org/abs/2102.05734">Tjoa et al.</a> 1c. Unruh-DeWitt Quantum Computers which utilize the spin qubits coupled to Luttinger liquids to create flying qubits; <a href="https://arxiv.org/abs/2210.12552">Aspling et al.</a></li> <li>Understanding spacetime entanglement and connections to black holes via ER = EPR; <a href="https://arxiv.org/abs/1306.0533">Maldacena and Susskind</a></li> <li>Emergent space and time from entanglement; <a href="https://arxiv.org/abs/1606.08444">Cao et al.</a></li> </ol> <p>There are many more topics in RQI, but this is a good starting place for the curious reader.</p> <h3 id="what-future-insights-will-rqi-give-">What future insights will RQI give? <a name="question9"></a></h3> <p>At this point, it’s hard to tell what the future of RQI holds. Given that the field is very young, there is much work to do as we move forward. The insights we gain promise a deeper understanding of the nature of space and time but are also used practically to probe quantum information in quantum materials. A fully fleshed-out theory of relativistic quantum information will include these and everything in the middle. For that, there is much to look forward to.</p>]]></content><author><name></name></author><category term="quantum"/><category term="information theory"/><summary type="html"><![CDATA[by Eric Aspling]]></summary></entry><entry><title type="html">Generating 2D Fingerprints to Predict Properties of Metal Organic Frameworks Using Machine Learning</title><link href="https://lawlergroup.lassp.cornell.edu/blog/2022/Predicting_properties_of_metal_organic_frameworks/" rel="alternate" type="text/html" title="Generating 2D Fingerprints to Predict Properties of Metal Organic Frameworks Using Machine Learning"/><published>2022-09-28T12:20:11+00:00</published><updated>2022-09-28T12:20:11+00:00</updated><id>https://lawlergroup.lassp.cornell.edu/blog/2022/Predicting_properties_of_metal_organic_frameworks</id><content type="html" xml:base="https://lawlergroup.lassp.cornell.edu/blog/2022/Predicting_properties_of_metal_organic_frameworks/"><![CDATA[<p align="center"> <img src="https://i.imgur.com/dmsDahh.png" width="75%"/> </p> <p>With the increase in global transportation and shipping, the need for efficient and safe gas transportation is more important than ever. One method of accomplishing this is through <a href="https://en.wikipedia.org/wiki/Metal-organic_framework">metal-organic frameworks</a> (MOFs). MOFs are a class of crystalline materials with extremely high porosity, inner surface area, and flexibility in network topologies. MOFs are composed of positively charged metal ions connected by organic linkers. This unique composition gives MOFs an incredibly large inner surface area ideal for storing or separating gases.</p> <p>Currently there are more than <a href="https://www.nanowerk.com/mof-metal-organic-framework.php">90,000 different types of MOFs</a> that scientists have synthesized. The problem is that trying to create new MOFs from scratch is time consuming and expensive. The physical process involves chemists combining many different chemicals in a beaker and letting the mixture crystalize for several hours or sometimes even <a href="https://www.intechopen.com/chapters/71021">days</a>. At the end of this process if they’re lucky, they’ll have a new MOF; otherwise, they’ll need to restart the whole process. By using machine learning to generate likely properties of MOFs based on the MOFs currently in existence, scientists can get a head start on synthesizing new ones.</p> <p>One aspect of this machine learning approach is trying to predict geometric properties of MOFs such as pore-limiting diameter and largest cavity diameter. Current calculations are done using Monte Carlo simulations which are time consuming and costly. The ability to have a trained model that can quickly verify if a MOF structure is geometrically similar to those that already exist would be advantageous.</p> <p>Our team at Binghamton University (Shehtab Zaman, Kenneth Chiu, Michael J. Lawler and myself) undertook the task of developing this model. We decided to use a 2-dimensional Convolutional Neural Network (CNN). More precisely the <a href="https://arxiv.org/abs/1512.00567v3">InceptionV3</a> architecture with transfer learning activated as seen in Fig. 1. We decided on this model because of its pre-trained nature and high-performance on ImageNet, a dataset of 1 million labeled images. The problem is that MOF coordinates are in 3-dimensional space so in order to utilize a 2D CNN we need a dimensionality reduction algorithm which can preserve the geometric properties that are represented in the 3D space.</p> <p>We did also attempt using a 3D-CNN, but because a substantial quantity of MOFs had atom counts in the thousands, a 3D-CNN would be largely inefficient. We also hoped to utilize the pre-trained latent space of the InceptionV3 architecture which excels at recognizing tiny differences in edges and features of the images.</p> <p align="center"> <img src="https://miro.medium.com/max/960/1*gqKM5V-uo2sMFFPDS84yJw.png" width="500"/> </p> <p align="center"> <b>Fig. 1 InceptionV3 Architecture. For transfer learning only the final part of the model is trained on the new data.</b> </p> <p>To solve this issue we applied multidimensional scaling (MDS) to the 3D coordinates. This resulted in a 2D matrix of the atom positions which hopefully preserves the geometric properties of the original 3D MOF. While some information is definitely lost from the 3D to 2D reduction, our hope is that it is small enough to not affect our predictions. The formula for classical MDS is as follows:</p> <p>\begin{equation} \label{mds} Stress<em>D(x_1,x_2,…,x_N) = \left(\sum</em>{x \neq j=1,…,N}(d_{ij}-||x_i-x_j||)^2\right)^{1/2} \end{equation}</p> <p>Where:</p> <p>\begin{equation} \label{d} d<em>{ij} = \sqrt{\sum^p</em>{k=1}\left(x<em>{ij}-x</em>{jk}\right)^2} \end{equation}</p> <p>And X is a configuration of points in low dimensional space p. D is a distance matrix that approximates the interpoint distances of X, and StressD is a residual sum of squares.</p> <p>We also tested MDS against other dimensional reduction methods, but we ultimately decided MDS was the best based on our testing with synthetic 3D spheres. Below is a figure of MDS on a 3D MOF sample:</p> <p align="center"> <img src="https://i.imgur.com/Dq3smEu.png" width="300" height="150"/> </p> <p>We used the CoRE MOF 2019 dataset with expanded geometric properties such as henry’s constant, and surface area for our testing. The dataset contains over 14,000 MOF samples which also makes transfer learning the preferred approach since it excels at problems with low training samples. We used a few different methods of parsing the MOFs with varied results.</p> <p align="center"><img src="https://i.imgur.com/Jsjq8ZF.png" width="500"/></p> <p>The first method was just passing in the direct MOF coordinates (right). This resulted in low prediction accuracy so we instead passed in the MOF with coordinates outside the unit cell concatenated on as well (middle). This had better results, but still not enough to make the model plausible. The last method we tried was passing in the distance matrix of the MOF to MDS (left). This had the best results with an average error of 85% and a median of 45%, but with an error that high we could not publish the results in an academic conference.</p> <p align="center"><img src="https://i.imgur.com/0ArYOSs.png" width="400"/></p> <p>We still felt like our work was worthwhile so we decided to showcase it in this report so that other researchers might be able to discover a new approach to our methods. Some potential ideas that we thought of, but did not pursue, were to use a 2D image of the full 3D MOF from different points of view with partial alpha values to hopefully represent areas of low density atom concentrations that the model could recognize. If any interested researchers would like to build off our work, it is available at: https://github.com/szaman19/Materials-Search.</p> <h3 id="acknowledgments">Acknowledgments</h3> <p>This material is based on work supported by the National Science Foundation under grant No. OAC-1940243.</p> <h3 id="bibliography">Bibliography</h3> <p>Papers with code—Inception-v3 explained. (n.d.). Retrieved March 24, 2022, from https://paperswithcode.com/method/inception-v3</p> <p>Berger, M. (n.d.). MOF Metal Organic Framework—Definition, fabrication and use. Nanowerk. Retrieved March 24, 2022, from https://www.nanowerk.com/mof-metal-organic-framework.php</p> <p>Han, Y., Yang, H., &amp; Guo, X. (2020). Synthesis Methods and Crystallization of MOFs. In (Ed.), Synthesis Methods and Crystallization. IntechOpen. https://doi.org/10.5772/intechopen.90435</p>]]></content><author><name></name></author><category term="materials science"/><category term="neuralnets"/><category term="dimensionality reduction"/><category term="transfer learning"/><summary type="html"><![CDATA[by Jacob Barkovitch]]></summary></entry><entry><title type="html">Introduction to information channels</title><link href="https://lawlergroup.lassp.cornell.edu/blog/2022/Quantum_channel/" rel="alternate" type="text/html" title="Introduction to information channels"/><published>2022-08-15T16:40:16+00:00</published><updated>2022-08-15T16:40:16+00:00</updated><id>https://lawlergroup.lassp.cornell.edu/blog/2022/Quantum_channel</id><content type="html" xml:base="https://lawlergroup.lassp.cornell.edu/blog/2022/Quantum_channel/"><![CDATA[<p>Information processing is part of our everyday life. From reading your alarm clock in the morning to your dinner decision this evening, you’re in a constant bath of information. Despite the ability to process this information via your senses, interpreting this information physically and mathematically can seem overwhelming. Nevertheless, understanding the laws of nature requires a rigorous and exhaustive approach to modeling these processes. Those who study information sciences, have developed a series of mathematical tools to help understand the inter-workings behind how information transfers from inputs to outputs. These transfers, are aptly named <em>information channels</em>.</p> <p>Scientists are not the only ones who use information channels. In fact, as alluded above, they are a part of your every day life. An obvious information channel may be entering your pin number into an ATM. In this process, transfer of information that was stored in your brain into the ATM, enabled the machine to grant access for managing your banking. To Quantum Information Scientists, the processes that take place on the smallest of scales, like the photons exiting the ATM screen and entering your eyes, have complicated yet fascinating information channels that processes classical and quantum information.</p> <p>Just as with other fields of physics, classical and quantum information channels follow separate rules. The quantum revolution of the 1920s brought with it a confusing perspective of how we are to view the governing mechanics of the micro-world. Up until that moment, nature could be described by a set of deterministic rules. Only limited by the complexity of each system and the ignorance of the observer. The quantum founding fathers showed that at the smallest of scales, nature is fundamentally determined by probabilities.</p> <p>The probabilistic nature of quantum mechanics would thus play a lofty role in how we understand information. Imagine entering your pin number into a quantum mechanical ATM, only to get rejected fifty percent of the time. Rejected not due to some noise or interference, but instead some inherent property that governs the interactions taking place inside the ATM. We can see that these two ATMs process different types of information and subsequently need to construct different types of information channels. However, certain properties of these information channels will be analogous to each other. Therefore, in this article, we dive first into a more familiar domain of classical information channels while preparing to understand the more complicated but coinciding quantum information channels.</p> <p>The aim of this article, is to introduce some of the tools necessary to understand the properties of information channels. While the mathematics is deep and complex, I do my best to keep it self-contained as much as possible, with the intent to offer a broader audience the ideas behind information theory. Regardless, readers can skip the math as it appears, without loss of generality. For those inclined to dig deeper into information channels, you will find these helpful [1,2].</p> <h3 id="classical-information-channels">Classical Information Channels</h3> <p>As mentioned previously, complexity of a classical system can shroud its deterministic properties. Probabilities are often deployed as a simplification to these complex systems. While there is nothing quantum about flipping a fair coin, the fact that quantum theory is probabilistic, allows us to draw similar allusions by following such a system. To unpack the information hidden in this random process of coin flipping, we need a theory to measure information as it changes through these classical processes. For that, we turn to Shannon’s[3] information theory.</p> <p>Shannon’s theory is best understood with the usage of <em>bits</em>. A bit is a single piece of information that describes the state of an object. Consider the above example of a fair coin. The state of the coin can be described by assigning the value of zero to heads and one to tails. Therefore, reading out the bit is equivalent to measuring the state of the coin. To measure the information of a system[4], we first consider the Shannon entropy \(H(X)\) given by,</p> <p>\begin{equation} \label{shannon entropy} H(X) = \sum_x -p_X(x)\log p_X(x) \end{equation}</p> <p>with probability distribution \(p_X(x)\). \(H(x)\) will measure the <em>randomness</em> or <em>surprise</em> in the output of the system. If we consider our above example of the fair coin we expect a Shannon Entropy of one, which is the maximum amount of uncertainty for this system. This can easily be seen by replacing the fair coin with a weighted coin. A weighted coin with two-thirds probability of heads and one-third probability of tails, has a Shannon Entropy of 0.918. A weighted coin with nine-tenths probability of heads and one-tenth tails will yields 0.469. As the coin gets more weighted, the uncertainty in the result of the coin flip goes down, and subsequently the measure of Shannon Entropy does as well. Generally, we can say if \(x\) is the probability of obtaining heads, the probability of obtaining tails is \(1-x\) and eq. \eqref{shannon entropy} becomes, \begin{equation} H(X) = x\log x + (1-x)\log(1-x). \end{equation}</p> <p>We can see in figure 1, that the information content of \(X\) is maximized when both the events are equally likely. This makes sense because having equal probability outcomes, makes the task of guessing the hardest. So in essence, the entropy is closely related to the contents of the information channel.</p> <p>For readers who enjoy the mathematics (others can skip this paragraph), Shannon entropy specifically measures information content of the random variable \(X\). If we refer to this “surprise” or “randomness” of an outcome \(x\) as the information content \(i(x)\), and define it as \(i(x) = \log(\frac{1}{p(x)}) = -\log(p(x))\). This measurement has the property that it is high for low probability outcomes and low for high probability outcomes. Moreover, this also has another desirable property of “additivity” i.e. the information content of two outcomes \(x\) and \(y\) get added since the joint probability \(p(x,y) = p(x).p(y) \implies i(x,y) = -\log(p(x)p(y)) = -\log(p(x)) - \log(p(y)) = i(x) + i(y)\). Thus, the total information content of the random variable \(X\) can be written as the weighted average of the information of possible outcomes.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/info_channel/coin_toss_entropy.png" sizes="95vw"/> <img src="/assets/img/info_channel/coin_toss_entropy.png" class="img-fluid rounded z-depth-1" width="300" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="figcaption"><em>Fig 1: Graphical representation of H(x) for our fair-coin flipping system. Notice the maximum entropy occurs when our probability \(x = 1/2\)</em></p> <p>For both the classical and the quantum case of information channels, we will model a very simple operation that has purpose both in classical and quantum computing. For the classical system we will use a <em>flip</em> operation. The flip operation will take our input bit and flip the value of it. For instance, our fair coin modeled with heads as zero and tails as one, will do just as it says. Flip the side of the coin from its initial configuration to the opposite. Figure 2 gives a pictorial representation of this process.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/info_channel/bit_flip.png" sizes="95vw"/> <img src="/assets/img/info_channel/bit_flip.png" class="img-fluid rounded z-depth-1" width="300" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="figcaption"><em>Fig 2: A simple classical information channel that shows a flip operation acting on a coin. The coin is initially in state of heads but the flip operation changes this to tails.</em></p> <p>A useful calculation in information theory is <em>Channel Capacity</em>, which is a measure of information loss (or conversely propagated) through our circuit. Information lost is often do to the noisiness of a channel. Noise is attributed to unwanted interactions and will affect the input states by altering them. For instance, a sender Alice and a receiver Bob may be attempting to communicate classically. Choosing a noisy channel to save resources could alter the inputs enough that Bob would be unable to decipher the original message Alice has sent. If instead of a fair coin, suppose Alice has access to a set of random variables \(X\). She encodes a message onto the elements of \(X\) and sends it via some noisy channel to Bob who receives a set of random variables \(Y\) to decipher. Our aim now must be to mathematically evaluate the channel that led from \(X\) to \(Y\). This will show if the channel reliably transferred Alice’s message. Pictorially this is shown in figure 3.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/info_channel/classical_circuit.png" sizes="95vw"/> <img src="/assets/img/info_channel/classical_circuit.png" class="img-fluid rounded z-depth-1" width="500" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="figcaption"><em>Fig 3: Alice’s message is encoded into the elements \(x_i\) where it is passed through some noisy channel \(\mathcal{N}\) and later deciphered by Bob.</em></p> <p>After the information transfer, Bob has access to \(Y\), and the amount of his randomness/surprise about \(X\) is quantified by the <em>Conditional Entropy</em>. Mathematically this has the shape, \begin{equation} \label{Conditional Entropy} H(Y|X) = -\sum<em>{x,y}p</em>{Y|X}(y|x)\log \left( \frac{p*{Y|X}(y|x)}{p_Y(y)} \right) \end{equation} where \(p*{X,Y}(x,y)\) is an expression of the multiplicity of probability namely, \(p_{X,Y}(x,y)=p_X(x)p_Y(y)\). Notice that \(H(X) \geq H(X|Y)\), and subsequently \(I(X;Y) \geq 0\). Here these probabilities are a result of the interactions within the noisy quantum channels. The measure of how much information Bob recovers, can be expressed as the <em>Mutual Information</em> \(I(X;Y)\), where \begin{equation}\label{Mutual Information} I(X;Y) = H(Y)-H(Y|X). \end{equation} The mutual information in essence quantifies the similarity or correlations between the random variables of Alice and Bob. This is because \(H(Y|X)\) quantifies the amount of error due to noise. In a perfect channel this value is zero because \(Y\) is fully aware of how \(X\) will change.</p> <p>Up until this point, we have very purposefully not discussed the contents of the noisy information channels. We want to have a means to describe an information channel either classically or quantum mechanically, but the contents of the channels may be messy and hard to understand. This is where <em>Channel Capacity</em> comes into play. Channel Capacity uses information to quantifiably measure the reliability of an information channel \(\mathcal{N}\). The means as to which the information passes through that channel is equivalent to knowledge of the information given and received. After all, the channel is a probabilistic mapping of the set \(X\) onto the set \(Y\), \begin{equation} \mathcal{N}:p_{Y|X}(y|x). \end{equation} Under this construction, we define the Channel Capacity as,</p> <p>\begin{equation} \mathcal{C}(\mathcal{N}) = \max<em>{p</em>{X}(x)}I(X;Y) \end{equation}</p> <p>which has a physical interpretation such that, if Alice can arrange her information in a way that allows a maximal amount of correlation with Bobs, then this is the upper bound to the capacity of the channel. Using what we learned about information, we can quantify the capacity of a channel which will help identify if these channels are efficient uses of resources.</p> <h3 id="quantum-information-channels">Quantum Information Channels</h3> <p>A quantum channel is a linear, completely positive, and trace preserving mapping corresponding to a physical evolution. This is a fancy way of saying that it too, is a probabilistic mapping from initial states to final states. Yet something deeper is going on here. The probabilities involved the information transfer is not necessarily due to our inability to evaluate a complex system. Instead, the mechanics are fundamentally probabilistic. This leads to a relationship deeper than correlation, <em>Entanglement</em>.</p> <p>Entanglement is the signature of a quantum information channel. We illustrate entanglement through a comparison to classical correlation. Suppose Alice and Bob are presented with two colored balls red and blue. The ball’s identities are concealed and Alice randomly selects one of the balls. Without looking, she departs for a far away star system. Upon arrival, Alice glances at her ball and notices the color is red. She can infer through correlation that Bob’s ball is blue. Now, suppose we repeat the experiment but, instead of balls we use perpetually spinning quantum coins. Ignoring how this is possible, let’s say that these quantum coins are arranged such that, if Alice measures heads, Bob’s coin will also measure heads. Similarly, if Alice measures tails, Bob’s will too! Alice ventures to the same far off system, quantum coin in hand, and measures her coin to which she instantly knows the measurement of Bob’s coin. The difference here is subtle. Our first experiment, involved a measurement early on, only random due to the ignorance of the observer, Alice. Throughout the voyage, Alice’s ball remained red, regardless of her knowledge. So to an outside observer, there was no surprise when she gets to her destination. A measurement of red was inevitable. The hidden information in the correlated state lived as classical information, only hidden by Alice’s inability to measure the color. Whereas, for experiment two, the states remained <strong>truly and fundamentally random</strong> up until the very moment Alice takes the measurement.</p> <p>The key to this entanglement, is the connection between the two quantum coins which we have conveniently hidden in the phrase ``Ignoring how this is possible”. Coincidentally, it is very possible in the realm of quantum mechanics. While, there is no classical analogy to explain why the coin-flip outcomes are correlated in this specific way, we don’t have to look far to find an example of this in nature. The spins of a pair of electrons can be entangled in such a fashion. If we prepare our electrons to be in an entangled state, measuring the spin of one electron can directly inform us of the spin of another. An instance of this in quantum computing is known as a “Bell Pair” and will come in handy in the proceding paragraphs.</p> <p>Entanglement, while mysterious in many ways, is a fundamental ingredient in quantum mechanics and is necessary for successful quantum computing. Similar to our classical bit, we introduce a quantum bit or <em>qubit</em>, that plays the same role as arbiter of information, but for quantum systems. If we assign the spin of the electron to a value of our qubit we can create a similar arrangement for information propagation. I should remind the reader that the math moving forward can seem a bit terse. Nevertheless, one should be able to skip the math and still gain an intuitive understanding of these concepts.</p> <p>For our purposes, lets assign spin up to our qubit \(\ket{0}\), and spin-down to \(\ket{1}\)\footnote{For those who are unfamiliar with this language. You need only associate the assignments behind the qubits for this article. While the math is rich and rewarding, it can be exhaustive and we will not be exploring that here.} We can then construct the state of our entangled pair (a Bell State), \begin{equation} \label{entangled state} \ket{\psi} = \frac{\ket{0}\ket{0} + \ket{1}\ket{1}}{\sqrt{2}} \end{equation} where the coefficient of \(\frac{1}{\sqrt{2}}\) guarantees that \(50\%\) of the time we will measure both spin-up and \(50\%\) of the time we will measure both spin down.</p> <p>Now that our descriptive states are set up as qubits, we can begin to evaluate the information in our states. Lets, first establish a parallel to our Shannon Entropy. It may surprise you that quantum mechanical entropy came first, this was accomplished by John von Neumann in 1932. Nevertheless, our <em>von Neumann entropy</em> has a similar form to the Shannon Entropy,</p> <p>\begin{equation} \label{von Neumann entropy} S(\rho) = -Tr(\rho\log \rho) \end{equation}</p> <p>where \(\rho\) is a density matrix whose purpose is to describe, in entirety, the probabilities and states of our system. If these states are eigenstates (eigenstates are states we can measure eg. spin-up and spin-down) then our density matrix takes the appearance of \(\rho = \sum_j n_j \ket{j}\bra{j}\) with \(\ket{j}\) denoting an eigenstate, and we can simplify equation \eqref{von Neumann entropy} to,</p> <p>\begin{equation} \label{von Neumann entropy eigenstates} S(\rho) = -\sum_j n_j \log n_j \end{equation}</p> <p>where \(n_j\) is the probability of measuring an eigenstate. But what does this quantify? In essence, this quantifies the <em>purity</em> of the state. While purity has some analogous meaning to classical information, we should press on with our development of quantum information and maybe a more intuitive understanding will present itself. By now you have probably noticed that equations \eqref{shannon entropy} and \eqref{von Neumann entropy eigenstates} are mostly identical. That changes quickly when we introduce <em>Quantum Mutual Information</em>.</p> <p>To understand Quantum Mutual Information, lets give Alice one qubit and Bob another. We can entangle them such that our input state matches that of equation \eqref{entangled state}. We can then model the mutual information between Alice’s and Bob’s qubits as,</p> <p>\begin{equation} I(A:B) = S(\rho<em>A) + S(\rho_B) - S(\rho</em>{AB}) \end{equation}</p> <p>where \(\rho_A = Tr_B (\rho)\) and \(\rho_{AB}\) is the density matrix describing the state of the Alice’s qubit. Tracing over part of the system, in essence ignores that part of the system. When we trace over Bob’s information we are focused solely on the information of Alice. The physical interpretation of mutual information is thus a quantifiable measurement of the entanglement shared between Alice and Bob. For a highly entangled state such as our bell state, we find a value of two which indicates a maximum entanglement between the two qubits. While quantum mutual information is insightful for understanding entanglement between states, these states remain as initial states. We have made a modification to our system between the classical and quantum cases. The modification is the usage of Bob. In our classical noisy channel arrangement, Bob received some data \(Y\) from Alice whose input was \(X\). The classical mutual information was a correlation between input and output. Therefore, We need another tool that is analogous to classical mutual information, that will evaluate how a quantum channel could alter this entanglement. Luckily, we’ll find a familiar analogy to classical mutual information.</p> <p>The flow of quantum information is quantifiable by how much entanglement passes from the initial state to the final state. <em>Quantum Coherent information</em> is a quantitative measurement of this transfer. To understand it better, lets bring in a third qubit belonging to Carla. Let’s suppose Alice and Carla entangle their qubits together and create a Bell Pair like that of eq. \eqref{entangled state}. Bob’s qubit is initially in a \(\ket{0}\) (spin-up) state with \(100\%\) probability. We now define a quantum channel from Alice to Bob where we aim to pass the entanglement. A popular quantum channel that is well known to preserve entanglement is done using a SWAP gate. The SWAP gate will do just as it says; swap the values of Alice’s and Bob’s qubits and with it the information associated. Figure 4 shows this quantum information channel, outlining the SWAP of entanglement from Alice’s qubit to Bob’s.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/info_channel/quantum_circuit.png" sizes="95vw"/> <img src="/assets/img/info_channel/quantum_circuit.png" class="img-fluid rounded z-depth-1" width="500" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="figcaption"><em>Fig 4: Alice and Carla initially have an entangled bell pair. After the SWAP gate is carried out Carla and Bob are now entangled and Alice is in the \(\ket{0}\) state.</em></p> <p>We can now define our quantum coherent information as,</p> <p>\begin{equation} I<em>C(\rho</em>{in,AB},\Xi) = S(\rho<em>{out,B})-S(\rho</em>{out,BC}). \end{equation} where, \(\rho_{in,AB}\) is the input to the channel and \(\Xi\) is the channel itself. One should note here, that the Channel \(\Xi\) is equivalent to \(\rho_{out,B}\). The difference in notations is that \(\Xi\) expresses the operators that make up the channel, and their acting on the initial states. Whereas, \(\rho_{out,B}\) is the output state which is a result of this channel. This is because \(\Xi\), mathematically is a mapping from initial states to final states. The analogy to the classical mutual information (Eq. \eqref{Mutual Information}) should be more evident now, and as we did in the classical case, we elevate this to the Quantum Channel Capacity by taking,</p> <p>\begin{equation} Q(\Xi) = \max<em>{\rho</em>{in,AB}}I<em>C(\rho</em>{in,AB},\Xi). \end{equation}</p> <p>Just as it was in classical information theory, the channel capacity is a measurement of reliability in transferring quantum information (ie. entanglement). So long as entanglement continues to be the best metric of a quantum computer, measuring the propagation of this entanglement, remains to be an important calculation as we further progress quantum computers.</p> <p>We have spoken in detail about two channels of information, yet there remains two more. Channels that change Classical information to Quantum information (This is a process that creates entanglement) and vice versa (Measurement). These channels are handled differently, and subsequently will have to wait until future writing.</p> <p>[1] M. M. Wilde, “Preface to the second edition,” <em>Quantum Information Theory</em>, p. xi–xii</p> <p>[2] L. Gyongyosi, S. Imre, and H. V. Nguyen, “A survey on quantum chan- nel capacities,” <em>IEEE Communications Surveys &amp; Tutorials</em>, vol. 20, no. 2, p. 1149-1205, 2018.</p> <p>[3] Named after Claude Shannon, a mathematician and engineer who is often referred to as the “father of information theory”.</p> <p>[4] We will be using \(\log_2\) for the entirety of this post for consistency.</p>]]></content><author><name></name></author><category term="quantum"/><category term="information theory"/><summary type="html"><![CDATA[by Eric Aspling]]></summary></entry><entry><title type="html">Do Quantum Circuits Outperform Neural Networks?</title><link href="https://lawlergroup.lassp.cornell.edu/blog/2021/Do-quantum-circuits-outperform-neural-networks/" rel="alternate" type="text/html" title="Do Quantum Circuits Outperform Neural Networks?"/><published>2021-10-13T16:40:16+00:00</published><updated>2021-10-13T16:40:16+00:00</updated><id>https://lawlergroup.lassp.cornell.edu/blog/2021/Do-quantum-circuits-outperform-neural-networks</id><content type="html" xml:base="https://lawlergroup.lassp.cornell.edu/blog/2021/Do-quantum-circuits-outperform-neural-networks/"><![CDATA[<p>Quantum machine learning has received a lot of hype. But there is a simple way to see if the hype is just hype: classical simulation! In an apples-to-apples comparison, can a quantum circuit outperform a classical neural network?</p> <p>Recently, I was reading this very nice article by Taylor Patti (Harvard, Nvidia) and collaborators (<a href="https://arxiv.org/pdf/2106.13304.pdf">see arXiv</a>) and they found a nice application of an idea I had: use pytorch to simulate quantum circuits. They managed to simulate a quantum circuit with 512 qubits that is 13 gates deep and were motivated to do so for an application with time reversal symmetry that allows them to use real numbers instead of complex ones which pytorch is not well setup to handle. The results from their GPU seem impressive and strengthens the motivation for this post: is a quantum circuit good at machine learning and for what applications is it good at? Combinatorial optimization?</p> <p>Of course, machine learning is a broad subject and neural networks are not the only machines around. There are many methods and nobody has created <a href="https://en.wikipedia.org/wiki/The_Master_Algorithm">Pedro Domingo’s “Master algorithm”</a>. It could be that say at image classification or some other common neural network application, a quantum circuit fails miserably compared to neural networks. But if this is the case, it doesn’t mean the quantum circuit is a poor machine in general, just for this one application. So a weaker version of this post’s question is: is there an application where a quantum circuit shines compared to other machine learning algorithms?</p> <p>A quantum circuit requires some background to understand. A quick definition is that a quantum circuit is a non-deterministic map from quantum data to classical bit strings. But for more details see for example <a href="https://qiskit.org/textbook/ch-algorithms/defining-quantum-circuits.html">qiskit’s discussion</a>, <a href="https://en.wikipedia.org/wiki/Quantum_circuit">wikipedia’s discussion</a>, or a good book on quantum computing that does not require a physics background such as <a href="https://michaelnielsen.org/qcqi/QINFO-book-nielsen-and-chuang-toc-and-chapter1-nov00.pdf">Nielsen and Chuang</a>. So there are two properties that make quantum circuits different from neural networks: they act on quantum data (a vector in a tensor product space \(V_d\otimes V_d\otimes\ldots\)) instead of \({\mathbb R}^N\) and produce a distribution of outputs for the same input.</p> <p>Here is some code I wrote to create a simple quantum circuit based machine in pytorch. The weight matrix is an orthogonal matrix generated by a QR decomposition. A measurement is performed on the first qubit and a single bit representing the outcome of the measurement is performed along with the collapsed state after measurement is returned.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">QC</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
<span class="k">def</span> <span class="err">**</span><span class="nf">init</span><span class="o">**</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">N</span><span class="p">):</span>
<span class="nf">super</span><span class="p">().</span><span class="o">**</span><span class="n">init</span><span class="o">**</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">N</span>

        <span class="n">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">([</span><span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">N</span><span class="p">)])</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">qr_correct_weights</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>

        <span class="n">self</span><span class="p">.</span><span class="n">w_indices</span> <span class="o">=</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">((</span><span class="nf">chr</span><span class="p">(</span><span class="mi">97</span><span class="o">+</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">N</span><span class="p">)))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">x_indices</span> <span class="o">=</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">((</span><span class="nf">chr</span><span class="p">(</span><span class="mi">97</span><span class="o">+</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">N</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_indices</span><span class="o">+</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">w_indices</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span>

        <span class="n">proj0_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_indices</span><span class="o">+</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="o">+</span><span class="sh">'</span><span class="s">az</span><span class="sh">'</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">]]))</span>
        <span class="n">proj1_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_indices</span><span class="o">+</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="o">+</span><span class="sh">'</span><span class="s">az</span><span class="sh">'</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">]]))</span>
        <span class="n">norm0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_indices</span><span class="o">+</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">x_indices</span><span class="p">,</span><span class="n">proj0_x</span><span class="p">,</span><span class="n">proj0_x</span><span class="p">)</span>
        <span class="n">norm1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">x_indices</span><span class="o">+</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">x_indices</span><span class="p">,</span><span class="n">proj1_x</span><span class="p">,</span><span class="n">proj1_x</span><span class="p">)</span>
        <span class="n">p0</span> <span class="o">=</span> <span class="n">norm0</span><span class="o">/</span><span class="p">(</span><span class="n">norm0</span><span class="o">+</span><span class="n">norm1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(())</span> <span class="o">&lt;</span> <span class="n">p0</span><span class="p">:</span>
            <span class="nf">return </span><span class="p">(</span><span class="sh">'</span><span class="s">0b0</span><span class="sh">'</span><span class="p">,</span><span class="n">proj0_x</span><span class="o">/</span><span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">p0</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nf">return </span><span class="p">(</span><span class="sh">'</span><span class="s">0b1</span><span class="sh">'</span><span class="p">,</span><span class="n">proj1_x</span><span class="o">/</span><span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p0</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">qr_correct_weights</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># We may need to call this every now and then to fix numerical errors (maybe once per epoch?)
</span>        <span class="n">Q</span><span class="p">,</span><span class="n">R</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">qr</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">view</span><span class="p">((</span><span class="mi">2</span><span class="o">**</span><span class="n">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span><span class="mi">2</span><span class="o">**</span><span class="n">self</span><span class="p">.</span><span class="n">N</span><span class="p">)))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span><span class="nd">@torch.diag</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sign</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">diag</span><span class="p">(</span><span class="n">R</span><span class="p">)))).</span><span class="nf">view</span><span class="p">([</span><span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">N</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">grad_symmetrize_</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># If gradient is a symmetric matrix, then it will preserve the orthogonality of the weights
</span>        <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span> <span class="o">+</span>
                                    <span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">permute</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">N</span><span class="o">+</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">N</span><span class="p">)]))</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">grad_symmetrize_</span><span class="p">()</span> <span class="c1"># This line doesn't seem to matter!</span></code></pre></figure> <p>It is a beginning only, but seems to show some promise. I was able to show the weight matrix remained orthogonal after a little training.</p> <p>Now to pose the real test we need a database of quantum data: a collection of vectors in a tensor product space and a problem associated with this data which can be solved either by a quantum circuit or a neural network (by flattening the tensor product space). In a future post, we may post an ideal data set or data sets for this purpose: one composed of high rank tensors found in quantum simulations of quantum materials. However, an alternative in the mean time might be to study 4D data such as 3D videos like <a href="http://www.scan-net.org">ScanNet</a>. Neural networks perform well on one (bar codes?) and two dimensional (images) data sets but they seem to struggle more as you go up in dimension. Perhaps this is an opportunity for machines built out of quantum circuits or inspiried by quantum circuits.</p> <p>I will proprose this problem one day for a student in my group, but if you find this idea interesting, feel please go ahead with your own “experiments” and let us know how it went!</p>]]></content><author><name></name></author><category term="quantum"/><category term="neuralnets"/><summary type="html"><![CDATA[by Michael Lawler]]></summary></entry><entry><title type="html">The art of quantum</title><link href="https://lawlergroup.lassp.cornell.edu/blog/2020/Why_think_quantum/" rel="alternate" type="text/html" title="The art of quantum"/><published>2020-05-29T20:15:21+00:00</published><updated>2020-05-29T20:15:21+00:00</updated><id>https://lawlergroup.lassp.cornell.edu/blog/2020/Why_think_quantum</id><content type="html" xml:base="https://lawlergroup.lassp.cornell.edu/blog/2020/Why_think_quantum/"><![CDATA[<h3 id="thought-of-the-day">Thought of the day:</h3> <p>A Quantum world is to modern art as our ordinary world is to classic art. In art, the modern form is obtained by changing the rules of the classic form and introduces surprising and unexpected results. Similarly a quantum world is like our ordinary world but with the rules changed. Indeed, if we don’t observe the quantum world, then it seems anything can happen much like modern art.</p> <p>The best known example of this anything-goes-until-you-measure phenomena is the wave function: a particle seems to exist in many places at the same time until you measure its position and find it located at a specific spot–the collapse of the wave function. However, does this extend to time? Can a particle exist at multiple points in time? The answer is yes! But, like the spatial case, it can’t violate the causality of classical physics upon measurement. Check out this interesting paper on the subject by <a href="https://arxiv.org/pdf/2101.09107.pdf">Purves and Short</a> at the university of Bristol.</p>]]></content><author><name></name></author><category term="quantum"/><category term="thought"/><summary type="html"><![CDATA[by Michael Lawler]]></summary></entry></feed>