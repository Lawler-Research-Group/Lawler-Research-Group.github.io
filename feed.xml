<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="https://lawlergroup.lassp.cornell.edu/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lawlergroup.lassp.cornell.edu/" rel="alternate" type="text/html" hreflang="en"/><updated>2022-10-30T23:51:00+00:00</updated><id>https://lawlergroup.lassp.cornell.edu/feed.xml</id><title type="html">Lawler Research Group</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The Relativistic Quantum Information FAQ</title><link href="https://lawlergroup.lassp.cornell.edu/blog/2022/The_Relativistic_Quantum_Information_FAQ/" rel="alternate" type="text/html" title="The Relativistic Quantum Information FAQ"/><published>2022-10-30T00:00:00+00:00</published><updated>2022-10-30T00:00:00+00:00</updated><id>https://lawlergroup.lassp.cornell.edu/blog/2022/The_Relativistic_Quantum_Information_FAQ</id><content type="html" xml:base="https://lawlergroup.lassp.cornell.edu/blog/2022/The_Relativistic_Quantum_Information_FAQ/"><![CDATA[<p>A recent visit to Johns Hopkins University and the University of Maryland has me realizing that Relativistic Quantum Information (RQI) is too young in its infancy for many to know what it’s all about. I received many questions during conversations with friends and colleagues about RQI. This FAQ is a collection of those questions and others that may prove helpful. Please feel free to add any questions to the comment section below, and I will do my best to answer them.</p> <h1 id="questions">Questions</h1> <ul> <li><a href="#question1">What is RQI?</a></li> <li><a href="#question2">What classifies as quantum information?</a></li> <li><a href="#question3">What is relativistic about RQI?</a></li> <li><a href="#question4">Why add relativity?</a></li> <li><a href="#question5">When did RQI become an area of study?</a></li> <li><a href="#question6">What is an Unruh-DeWitt detector?</a></li> <li><a href="#question7">What other methods are used to study RQI?</a></li> <li><a href="#question8">What are current RQI physicists studying?</a></li> <li><a href="#question9">What future insights will RQI give?</a></li> </ul> <h3 id="what-is-rqi--">What is RQI? <a name="question1"> </a></h3> <p>RQI is a field of physics that combines the formalism of quantum information with relativity theory to understand how entanglement is transferred, created, and propagated in a relativistic setting.</p> <h3 id="what-classifies-as-quantum-information-">What classifies as quantum information? <a name="question2"></a></h3> <p>Information is best thought of as the measurement of a state. The word <strong>measurement</strong> has an important role to play here as there are many different measures of individual states (e.g., temperature, velocity, spin, etc.). As a quick aside, a state is not necessarily quantum. It could be the state of a system or a state of being, all of which carry information. Nevertheless, this is how we describe information.</p> <p>There are two forms of information; classical and quantum, but a defining feature that separates the two is <strong>entanglement</strong>. Quantum information is specific in that the measurement it’s concerned with is entanglement. Of course, there are many measures of entanglement between quantum systems, but to be considered quantum information, it must contain entanglement. All other information classifies as classical. I recently wrote a <a href="https://lawlergroup.lassp.cornell.edu/blog/2022/Quantum_channel/">blog post</a> on evaluating different types of information. If you are interested in a deeper dive, please check out that post and the references therein.</p> <h3 id="what-is-relativistic-about-rqi-">What is relativistic about RQI? <a name="question3"></a></h3> <p>The answer to this question is a bit tricky. The theory of RQI aims to tackle what we understand about quantum information, but in various ways that include relativity. To accomplish this, we use the language that handles quantum mechanics and special relativity; quantum field theory (QFT). However, QFT is a vast field. Specifically, the systems that I study, topological edge states of strongly correlated electrons, don’t <em>necessarily</em> include relativistic effects. Yet the machinery is utilized to great success and is equipped to handle relativistic velocities of Dirac fermions. Read on to find more (relativistic) use cases.</p> <h3 id="why-add-relativity-">Why add relativity? <a name="question4"></a></h3> <p>Regardless of the motivation, this has repeatedly been the natural tendency of a physical theory. First, in the early 1900s, mechanics was revolutionized by Einstein, as well as others. Then later, in the 1930s and ’40s, quantum mechanics was transformed into QFT by Dirac, Feynman, Schwinger, and more. This year (2022) brought the recognition of quantum information (QI) theory by Charles Bennet and Peter Shor et al. through various prizes, such as the 2023 Breakthrough Prize. Subsequently, the formalization of the theory in the mid to late 1990s wasn’t the first attempt to understand QI but instead marked the birth of the field. One may surmise that the transformation to a theory of RQI was inevitable, and as such, we have gained many more insights.</p> <h3 id="when-did-rqi-become-an-area-of-study-">When did RQI become an area of study? <a name="question5"></a></h3> <p>Similar to QI, RQI has been around for some time before the theory was laid out. It’s hard to pin down when people began thinking about relativistic entanglement. But it was certainly no later than the work of <a href="https://arxiv.org/abs/quant-ph/0302179">Alsing and Millburn</a>(2003) and formalized by <a href="https://arxiv.org/abs/1106.0280">Eduardo Martin-Martinez</a>(2011). Shortly after, a boom in the field took place with the introduction of the <a href="https://arxiv.org/abs/1207.3123">amps paradox</a>(2012) as well as the work developed by <a href="https://arxiv.org/abs/1306.0533">Maldacena and Susskind</a>(2013) in their famous ER=EPR paper.</p> <p>Eduardo’s approach to formalizing the theory was to calculate quantum information values utilizing Unruh-DeWitt detectors as the machinery to transfer entanglement to quantum fields via interactions, encoding (decoding) QI onto (off of) quantum fields in a process known as entanglement harvesting. Subsequent years have led to progress in understanding the requirements to carry out this type of quantum channel, and <a href="https://arxiv.org/abs/2210.12552">recent work</a> by John Marohn, Michael Lawler, and myself have connected this idea to experiment.</p> <h3 id="what-is-an-unruh-dewitt-detector-">What is an Unruh-DeWitt detector? <a name="question6"></a></h3> <p>UDW detectors were proposed as a thought experiment in Bill Unruh’s <a href="https://journals.aps.org/prd/abstract/10.1103/PhysRevD.14.870">1976 paper</a> “Notes on Black-hole Evaporation”. In this paper, Unruh imagined a detector with two states whose reading determined if radiation was present. These detectors, as he imagined, would be accelerated and subsequently detect radiation not present in an inertial frame of reference. This radiation became known as Unruh radiation.</p> <p>Eventually, the machinery was found promising for entanglement harvesting, and today RQI physicists use UDWs to understand particle interactions in curved space-time. One <a href="https://arxiv.org/abs/1908.07523">recent result</a> was the impracticality of wireless quantum communication by Simidzija et al. who showed that to decode the information from a quantum field, the detector would need to exist on the entirety of the light-cone that stems from the encoding interaction. This unreasonable requirement is what prevents wireless quantum communication.</p> <h3 id="what-other-methods-are-used-to-study-rqi-">What other methods are used to study RQI? <a name="question7"></a></h3> <p>As with QI, RQI has many subfields that most may not realize are studies of RQI. Two of the more famous problems tackled with RQI are ER=EPR and Emergent spacetime. ER=EPR links two famous papers of Einstein and, although controversial, ER=EPR tells a tale of entangled black holes linked together by a wormhole, which has consequences for the entangled connections of spacetime around the horizons of the black holes.</p> <p>Recently I was introduced to studies on emergent spacetime. Take, for example, this 2016 <a href="https://arxiv.org/abs/1606.08444">paper</a> by ChunJun Cao, Sean Carroll, and Spyridon Michalakis as well as the associated <a href="https://www.preposterousuniverse.com/blog/2016/07/18/space-emerging-from-quantum-mechanics/">blog post</a>. Carroll explains the result of this work as follows,</p> <p>“<em>The claim, in its most dramatic-sounding form, is that gravity (spacetime curvature caused by energy/momentum) isn’t hard to obtain in quantum mechanics — it’s automatic! Or at least, the most natural thing to expect. If geometry is defined by entanglement and quantum information, then perturbing the state (e.g. by adding energy) naturally changes that geometry. And if the model matches onto an emergent field theory at large distances, the most natural relationship between energy and curvature is given by Einstein’s equation.</em>”</p> <p>As admitted by Carroll in the post, this work is controversial and speculative. However, these two areas, as well as others, are recent and exciting fields that utilize relativity and quantum information to shed light on the fundamental underpinnings of the universe.</p> <h3 id="what-are-current-rqi-physicists-studying-">What are current RQI physicists studying? <a name="question8"></a></h3> <p>The areas discussed in this FAQ are all topics modern RQI scientists are thinking about, but in case you skipped directly to this question, let’s do a quick review with resources.</p> <ol> <li> <p>Unruh-DeWitt detectors (This is a bit biased as this is my field of study) 1a. UDW detectors used to understand wireless quantum communication; <a href="https://arxiv.org/abs/1908.07523">Simidzija et al.</a> 1b. UDW detectors to understand interactions in curved spacetime; <a href="https://arxiv.org/abs/2102.05734">Tjoa et al.</a> 1c. Unruh-DeWitt Quantum Computers which utilize the spin qubits coupled to Luttinger liquids to create flying qubits; <a href="https://arxiv.org/abs/2210.12552">Aspling et al.</a></p> </li> <li>Understanding spacetime entanglement and connections to black holes via ER = EPR; <a href="https://arxiv.org/abs/1306.0533">Maldacena and Susskind</a></li> <li>Emergent space and time from entanglement; <a href="https://arxiv.org/abs/1606.08444">Cao et al.</a></li> </ol> <p>There are many more topics in RQI, but this is a good starting place for the curious reader.</p> <h3 id="what-future-insights-will-rqi-give-">What future insights will RQI give? <a name="question9"></a></h3> <p>At this point, it’s hard to tell what the future of RQI holds. Given that the field is very young, there is much work to do as we move forward. The insights we gain promise a deeper understanding of the nature of space and time but are also used practically to probe quantum information in quantum materials. A fully fleshed-out theory of relativistic quantum information will include these and everything in the middle. For that, there is much to look forward to.</p>]]></content><author><name></name></author><category term="quantum,"/><category term="informationtheory"/><summary type="html"><![CDATA[by Eric Aspling]]></summary></entry><entry><title type="html">Generating 2D Fingerprints to Predict Properties of Metal Organic Frameworks Using Machine Learning</title><link href="https://lawlergroup.lassp.cornell.edu/blog/2022/Predicting_properties_of_metal_organic_frameworks/" rel="alternate" type="text/html" title="Generating 2D Fingerprints to Predict Properties of Metal Organic Frameworks Using Machine Learning"/><published>2022-09-28T12:20:11+00:00</published><updated>2022-09-28T12:20:11+00:00</updated><id>https://lawlergroup.lassp.cornell.edu/blog/2022/Predicting_properties_of_metal_organic_frameworks</id><content type="html" xml:base="https://lawlergroup.lassp.cornell.edu/blog/2022/Predicting_properties_of_metal_organic_frameworks/"><![CDATA[<p align="center"> <img src="https://i.imgur.com/dmsDahh.png" width="75%"/> </p> <p>With the increase in global transportation and shipping, the need for efficient and safe gas transportation is more important than ever. One method of accomplishing this is through <a href="https://en.wikipedia.org/wiki/Metal-organic_framework">metal-organic frameworks</a> (MOFs). MOFs are a class of crystalline materials with extremely high porosity, inner surface area, and flexibility in network topologies. MOFs are composed of positively charged metal ions connected by organic linkers. This unique composition gives MOFs an incredibly large inner surface area ideal for storing or separating gases.</p> <p>Currently there are more than <a href="https://www.nanowerk.com/mof-metal-organic-framework.php">90,000 different types of MOFs</a> that scientists have synthesized. The problem is that trying to create new MOFs from scratch is time consuming and expensive. The physical process involves chemists combining many different chemicals in a beaker and letting the mixture crystalize for several hours or sometimes even <a href="https://www.intechopen.com/chapters/71021">days</a>. At the end of this process if they’re lucky, they’ll have a new MOF; otherwise, they’ll need to restart the whole process. By using machine learning to generate likely properties of MOFs based on the MOFs currently in existence, scientists can get a head start on synthesizing new ones.</p> <p>One aspect of this machine learning approach is trying to predict geometric properties of MOFs such as pore-limiting diameter and largest cavity diameter. Current calculations are done using Monte Carlo simulations which are time consuming and costly. The ability to have a trained model that can quickly verify if a MOF structure is geometrically similar to those that already exist would be advantageous.</p> <p>Our team at Binghamton University (Shehtab Zaman, Kenneth Chiu, Michael J. Lawler and myself) undertook the task of developing this model. We decided to use a 2-dimensional Convolutional Neural Network (CNN). More precisely the <a href="https://arxiv.org/abs/1512.00567v3">InceptionV3</a> architecture with transfer learning activated as seen in Fig. 1. We decided on this model because of its pre-trained nature and high-performance on ImageNet, a dataset of 1 million labeled images. The problem is that MOF coordinates are in 3-dimensional space so in order to utilize a 2D CNN we need a dimensionality reduction algorithm which can preserve the geometric properties that are represented in the 3D space.</p> <p>We did also attempt using a 3D-CNN, but because a substantial quantity of MOFs had atom counts in the thousands, a 3D-CNN would be largely inefficient. We also hoped to utilize the pre-trained latent space of the InceptionV3 architecture which excels at recognizing tiny differences in edges and features of the images.</p> <p align="center"> <img src="https://miro.medium.com/max/960/1*gqKM5V-uo2sMFFPDS84yJw.png" width="500"/> </p> <p align="center"> <b>Fig. 1 InceptionV3 Architecture. For transfer learning only the final part of the model is trained on the new data.</b> </p> <p>To solve this issue we applied multidimensional scaling (MDS) to the 3D coordinates. This resulted in a 2D matrix of the atom positions which hopefully preserves the geometric properties of the original 3D MOF. While some information is definitely lost from the 3D to 2D reduction, our hope is that it is small enough to not affect our predictions. The formula for classical MDS is as follows:</p> <p>\begin{equation} \label{mds} Stress_D(x_1,x_2,…,x_N) = \left(\sum_{x \neq j=1,…,N}(d_{ij}-||x_i-x_j||)^2\right)^{1/2} \end{equation}</p> <p>Where:</p> <p>\begin{equation} \label{d} d_{ij} = \sqrt{\sum^p_{k=1}\left(x_{ij}-x_{jk}\right)^2} \end{equation}</p> <p>And X is a configuration of points in low dimensional space p. D is a distance matrix that approximates the interpoint distances of X, and StressD is a residual sum of squares.</p> <p>We also tested MDS against other dimensional reduction methods, but we ultimately decided MDS was the best based on our testing with synthetic 3D spheres. Below is a figure of MDS on a 3D MOF sample:</p> <p align="center"> <img src="https://i.imgur.com/Dq3smEu.png" width="300" height="150"/> </p> <p>We used the CoRE MOF 2019 dataset with expanded geometric properties such as henry’s constant, and surface area for our testing. The dataset contains over 14,000 MOF samples which also makes transfer learning the preferred approach since it excels at problems with low training samples. We used a few different methods of parsing the MOFs with varied results.</p> <p align="center"><img src="https://i.imgur.com/Jsjq8ZF.png" width="500"/></p> <p>The first method was just passing in the direct MOF coordinates (right). This resulted in low prediction accuracy so we instead passed in the MOF with coordinates outside the unit cell concatenated on as well (middle). This had better results, but still not enough to make the model plausible. The last method we tried was passing in the distance matrix of the MOF to MDS (left). This had the best results with an average error of 85% and a median of 45%, but with an error that high we could not publish the results in an academic conference.</p> <p align="center"><img src="https://i.imgur.com/0ArYOSs.png" width="400"/></p> <p>We still felt like our work was worthwhile so we decided to showcase it in this report so that other researchers might be able to discover a new approach to our methods. Some potential ideas that we thought of, but did not pursue, were to use a 2D image of the full 3D MOF from different points of view with partial alpha values to hopefully represent areas of low density atom concentrations that the model could recognize. If any interested researchers would like to build off our work, it is available at: https://github.com/szaman19/Materials-Search.</p> <h3 id="acknowledgments">Acknowledgments</h3> <p>This material is based on work supported by the National Science Foundation under grant No. OAC-1940243.</p> <h3 id="bibliography">Bibliography</h3> <p>Papers with code—Inception-v3 explained. (n.d.). Retrieved March 24, 2022, from https://paperswithcode.com/method/inception-v3</p> <p>Berger, M. (n.d.). MOF Metal Organic Framework—Definition, fabrication and use. Nanowerk. Retrieved March 24, 2022, from https://www.nanowerk.com/mof-metal-organic-framework.php</p> <p>Han, Y., Yang, H., &amp; Guo, X. (2020). Synthesis Methods and Crystallization of MOFs. In (Ed.), Synthesis Methods and Crystallization. IntechOpen. https://doi.org/10.5772/intechopen.90435</p>]]></content><author><name></name></author><category term="materials-science,"/><category term="neuralnets,"/><category term="dimensionality-reduction,"/><category term="transfer-learning"/><summary type="html"><![CDATA[by Jacob Barkovitch]]></summary></entry><entry><title type="html">Introduction to information channels</title><link href="https://lawlergroup.lassp.cornell.edu/blog/2022/Quantum_channel/" rel="alternate" type="text/html" title="Introduction to information channels"/><published>2022-08-15T16:40:16+00:00</published><updated>2022-08-15T16:40:16+00:00</updated><id>https://lawlergroup.lassp.cornell.edu/blog/2022/Quantum_channel</id><content type="html" xml:base="https://lawlergroup.lassp.cornell.edu/blog/2022/Quantum_channel/"><![CDATA[<p>Information processing is part of our everyday life. From reading your alarm clock in the morning to your dinner decision this evening, you’re in a constant bath of information. Despite the ability to process this information via your senses, interpreting this information physically and mathematically can seem overwhelming. Nevertheless, understanding the laws of nature requires a rigorous and exhaustive approach to modeling these processes. Those who study information sciences, have developed a series of mathematical tools to help understand the inter-workings behind how information transfers from inputs to outputs. These transfers, are aptly named <em>information channels</em>.</p> <p>Scientists are not the only ones who use information channels. In fact, as alluded above, they are a part of your every day life. An obvious information channel may be entering your pin number into an ATM. In this process, transfer of information that was stored in your brain into the ATM, enabled the machine to grant access for managing your banking. To Quantum Information Scientists, the processes that take place on the smallest of scales, like the photons exiting the ATM screen and entering your eyes, have complicated yet fascinating information channels that processes classical and quantum information.</p> <p>Just as with other fields of physics, classical and quantum information channels follow separate rules. The quantum revolution of the 1920s brought with it a confusing perspective of how we are to view the governing mechanics of the micro-world. Up until that moment, nature could be described by a set of deterministic rules. Only limited by the complexity of each system and the ignorance of the observer. The quantum founding fathers showed that at the smallest of scales, nature is fundamentally determined by probabilities.</p> <p>The probabilistic nature of quantum mechanics would thus play a lofty role in how we understand information. Imagine entering your pin number into a quantum mechanical ATM, only to get rejected fifty percent of the time. Rejected not due to some noise or interference, but instead some inherent property that governs the interactions taking place inside the ATM. We can see that these two ATMs process different types of information and subsequently need to construct different types of information channels. However, certain properties of these information channels will be analogous to each other. Therefore, in this article, we dive first into a more familiar domain of classical information channels while preparing to understand the more complicated but coinciding quantum information channels.</p> <p>The aim of this article, is to introduce some of the tools necessary to understand the properties of information channels. While the mathematics is deep and complex, I do my best to keep it self-contained as much as possible, with the intent to offer a broader audience the ideas behind information theory. Regardless, readers can skip the math as it appears, without loss of generality. For those inclined to dig deeper into information channels, you will find these helpful [1,2].</p> <h3 id="classical-information-channels">Classical Information Channels</h3> <p>As mentioned previously, complexity of a classical system can shroud its deterministic properties. Probabilities are often deployed as a simplification to these complex systems. While there is nothing quantum about flipping a fair coin, the fact that quantum theory is probabilistic, allows us to draw similar allusions by following such a system. To unpack the information hidden in this random process of coin flipping, we need a theory to measure information as it changes through these classical processes. For that, we turn to Shannon’s[3] information theory.</p> <p>Shannon’s theory is best understood with the usage of <em>bits</em>. A bit is a single piece of information that describes the state of an object. Consider the above example of a fair coin. The state of the coin can be described by assigning the value of zero to heads and one to tails. Therefore, reading out the bit is equivalent to measuring the state of the coin. To measure the information of a system[4], we first consider the Shannon entropy \(H(X)\) given by,</p> <p>\begin{equation} \label{shannon entropy} H(X) = \sum_x -p_X(x)\log p_X(x) \end{equation}</p> <p>with probability distribution \(p_X(x)\). \(H(x)\) will measure the <em>randomness</em> or <em>surprise</em> in the output of the system. If we consider our above example of the fair coin we expect a Shannon Entropy of one, which is the maximum amount of uncertainty for this system. This can easily be seen by replacing the fair coin with a weighted coin. A weighted coin with two-thirds probability of heads and one-third probability of tails, has a Shannon Entropy of 0.918. A weighted coin with nine-tenths probability of heads and one-tenth tails will yields 0.469. As the coin gets more weighted, the uncertainty in the result of the coin flip goes down, and subsequently the measure of Shannon Entropy does as well. Generally, we can say if \(x\) is the probability of obtaining heads, the probability of obtaining tails is \(1-x\) and eq. \eqref{shannon entropy} becomes, \begin{equation} H(X) = x\log x + (1-x)\log(1-x). \end{equation}</p> <p>We can see in figure 1, that the information content of \(X\) is maximized when both the events are equally likely. This makes sense because having equal probability outcomes, makes the task of guessing the hardest. So in essence, the entropy is closely related to the contents of the information channel.</p> <p>For readers who enjoy the mathematics (others can skip this paragraph), Shannon entropy specifically measures information content of the random variable \(X\). If we refer to this “surprise” or “randomness” of an outcome \(x\) as the information content \(i(x)\), and define it as \(i(x) = \log(\frac{1}{p(x)}) = -\log(p(x))\). This measurement has the property that it is high for low probability outcomes and low for high probability outcomes. Moreover, this also has another desirable property of “additivity” i.e. the information content of two outcomes \(x\) and \(y\) get added since the joint probability \(p(x,y) = p(x).p(y) \implies i(x,y) = -\log(p(x)p(y)) = -\log(p(x)) - \log(p(y)) = i(x) + i(y)\). Thus, the total information content of the random variable \(X\) can be written as the weighted average of the information of possible outcomes.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/info_channel/coin_toss_entropy-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/info_channel/coin_toss_entropy-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/info_channel/coin_toss_entropy-1400.webp"/> <img src="/assets/img/info_channel/coin_toss_entropy.png" class="img-fluid rounded z-depth-1" width="300" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="figcaption"><em>Fig 1: Graphical representation of H(x) for our fair-coin flipping system. Notice the maximum entropy occurs when our probability \(x = 1/2\)</em></p> <p>For both the classical and the quantum case of information channels, we will model a very simple operation that has purpose both in classical and quantum computing. For the classical system we will use a <em>flip</em> operation. The flip operation will take our input bit and flip the value of it. For instance, our fair coin modeled with heads as zero and tails as one, will do just as it says. Flip the side of the coin from its initial configuration to the opposite. Figure 2 gives a pictorial representation of this process.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/info_channel/bit_flip-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/info_channel/bit_flip-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/info_channel/bit_flip-1400.webp"/> <img src="/assets/img/info_channel/bit_flip.png" class="img-fluid rounded z-depth-1" width="300" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="figcaption"><em>Fig 2: A simple classical information channel that shows a flip operation acting on a coin. The coin is initially in state of heads but the flip operation changes this to tails.</em></p> <p>A useful calculation in information theory is <em>Channel Capacity</em>, which is a measure of information loss (or conversely propagated) through our circuit. Information lost is often do to the noisiness of a channel. Noise is attributed to unwanted interactions and will affect the input states by altering them. For instance, a sender Alice and a receiver Bob may be attempting to communicate classically. Choosing a noisy channel to save resources could alter the inputs enough that Bob would be unable to decipher the original message Alice has sent. If instead of a fair coin, suppose Alice has access to a set of random variables \(X\). She encodes a message onto the elements of \(X\) and sends it via some noisy channel to Bob who receives a set of random variables \(Y\) to decipher. Our aim now must be to mathematically evaluate the channel that led from \(X\) to \(Y\). This will show if the channel reliably transferred Alice’s message. Pictorially this is shown in figure 3.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/info_channel/classical_circuit-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/info_channel/classical_circuit-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/info_channel/classical_circuit-1400.webp"/> <img src="/assets/img/info_channel/classical_circuit.png" class="img-fluid rounded z-depth-1" width="500" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="figcaption"><em>Fig 3: Alice’s message is encoded into the elements \(x_i\) where it is passed through some noisy channel \(\mathcal{N}\) and later deciphered by Bob.</em></p> <p>After the information transfer, Bob has access to \(Y\), and the amount of his randomness/surprise about \(X\) is quantified by the <em>Conditional Entropy</em>. Mathematically this has the shape, \begin{equation} \label{Conditional Entropy} H(Y|X) = -\sum_{x,y}p_{Y|X}(y|x)\log \left( \frac{p_{Y|X}(y|x)}{p_Y(y)} \right) \end{equation} where \(p_{X,Y}(x,y)\) is an expression of the multiplicity of probability namely, \(p_{X,Y}(x,y)=p_X(x)p_Y(y)\). Notice that \(H(X) \geq H(X|Y)\), and subsequently \(I(X;Y) \geq 0\). Here these probabilities are a result of the interactions within the noisy quantum channels. The measure of how much information Bob recovers, can be expressed as the <em>Mutual Information</em> \(I(X;Y)\), where \begin{equation}\label{Mutual Information} I(X;Y) = H(Y)-H(Y|X). \end{equation} The mutual information in essence quantifies the similarity or correlations between the random variables of Alice and Bob. This is because \(H(Y|X)\) quantifies the amount of error due to noise. In a perfect channel this value is zero because \(Y\) is fully aware of how \(X\) will change.</p> <p>Up until this point, we have very purposefully not discussed the contents of the noisy information channels. We want to have a means to describe an information channel either classically or quantum mechanically, but the contents of the channels may be messy and hard to understand. This is where <em>Channel Capacity</em> comes into play. Channel Capacity uses information to quantifiably measure the reliability of an information channel \(\mathcal{N}\). The means as to which the information passes through that channel is equivalent to knowledge of the information given and received. After all, the channel is a probabilistic mapping of the set \(X\) onto the set \(Y\), \begin{equation} \mathcal{N}:p_{Y|X}(y|x). \end{equation} Under this construction, we define the Channel Capacity as,</p> <p>\begin{equation} \mathcal{C}(\mathcal{N}) = \max_{p_{X}(x)}I(X;Y) \end{equation}</p> <p>which has a physical interpretation such that, if Alice can arrange her information in a way that allows a maximal amount of correlation with Bobs, then this is the upper bound to the capacity of the channel. Using what we learned about information, we can quantify the capacity of a channel which will help identify if these channels are efficient uses of resources.</p> <h3 id="quantum-information-channels">Quantum Information Channels</h3> <p>A quantum channel is a linear, completely positive, and trace preserving mapping corresponding to a physical evolution. This is a fancy way of saying that it too, is a probabilistic mapping from initial states to final states. Yet something deeper is going on here. The probabilities involved the information transfer is not necessarily due to our inability to evaluate a complex system. Instead, the mechanics are fundamentally probabilistic. This leads to a relationship deeper than correlation, <em>Entanglement</em>.</p> <p>Entanglement is the signature of a quantum information channel. We illustrate entanglement through a comparison to classical correlation. Suppose Alice and Bob are presented with two colored balls red and blue. The ball’s identities are concealed and Alice randomly selects one of the balls. Without looking, she departs for a far away star system. Upon arrival, Alice glances at her ball and notices the color is red. She can infer through correlation that Bob’s ball is blue. Now, suppose we repeat the experiment but, instead of balls we use perpetually spinning quantum coins. Ignoring how this is possible, let’s say that these quantum coins are arranged such that, if Alice measures heads, Bob’s coin will also measure heads. Similarly, if Alice measures tails, Bob’s will too! Alice ventures to the same far off system, quantum coin in hand, and measures her coin to which she instantly knows the measurement of Bob’s coin. The difference here is subtle. Our first experiment, involved a measurement early on, only random due to the ignorance of the observer, Alice. Throughout the voyage, Alice’s ball remained red, regardless of her knowledge. So to an outside observer, there was no surprise when she gets to her destination. A measurement of red was inevitable. The hidden information in the correlated state lived as classical information, only hidden by Alice’s inability to measure the color. Whereas, for experiment two, the states remained <strong>truly and fundamentally random</strong> up until the very moment Alice takes the measurement.</p> <p>The key to this entanglement, is the connection between the two quantum coins which we have conveniently hidden in the phrase ``Ignoring how this is possible”. Coincidentally, it is very possible in the realm of quantum mechanics. While, there is no classical analogy to explain why the coin-flip outcomes are correlated in this specific way, we don’t have to look far to find an example of this in nature. The spins of a pair of electrons can be entangled in such a fashion. If we prepare our electrons to be in an entangled state, measuring the spin of one electron can directly inform us of the spin of another. An instance of this in quantum computing is known as a “Bell Pair” and will come in handy in the proceding paragraphs.</p> <p>Entanglement, while mysterious in many ways, is a fundamental ingredient in quantum mechanics and is necessary for successful quantum computing. Similar to our classical bit, we introduce a quantum bit or <em>qubit</em>, that plays the same role as arbiter of information, but for quantum systems. If we assign the spin of the electron to a value of our qubit we can create a similar arrangement for information propagation. I should remind the reader that the math moving forward can seem a bit terse. Nevertheless, one should be able to skip the math and still gain an intuitive understanding of these concepts.</p> <p>For our purposes, lets assign spin up to our qubit \(\ket{0}\), and spin-down to \(\ket{1}\)\footnote{For those who are unfamiliar with this language. You need only associate the assignments behind the qubits for this article. While the math is rich and rewarding, it can be exhaustive and we will not be exploring that here.} We can then construct the state of our entangled pair (a Bell State), \begin{equation} \label{entangled state} \ket{\psi} = \frac{\ket{0}\ket{0} + \ket{1}\ket{1}}{\sqrt{2}} \end{equation} where the coefficient of \(\frac{1}{\sqrt{2}}\) guarantees that \(50\%\) of the time we will measure both spin-up and \(50\%\) of the time we will measure both spin down.</p> <p>Now that our descriptive states are set up as qubits, we can begin to evaluate the information in our states. Lets, first establish a parallel to our Shannon Entropy. It may surprise you that quantum mechanical entropy came first, this was accomplished by John von Neumann in 1932. Nevertheless, our <em>von Neumann entropy</em> has a similar form to the Shannon Entropy,</p> <p>\begin{equation} \label{von Neumann entropy} S(\rho) = -Tr(\rho\log \rho) \end{equation}</p> <p>where \(\rho\) is a density matrix whose purpose is to describe, in entirety, the probabilities and states of our system. If these states are eigenstates (eigenstates are states we can measure eg. spin-up and spin-down) then our density matrix takes the appearance of \(\rho = \sum_j n_j \ket{j}\bra{j}\) with \(\ket{j}\) denoting an eigenstate, and we can simplify equation \eqref{von Neumann entropy} to,</p> <p>\begin{equation} \label{von Neumann entropy eigenstates} S(\rho) = -\sum_j n_j \log n_j \end{equation}</p> <p>where \(n_j\) is the probability of measuring an eigenstate. But what does this quantify? In essence, this quantifies the <em>purity</em> of the state. While purity has some analogous meaning to classical information, we should press on with our development of quantum information and maybe a more intuitive understanding will present itself. By now you have probably noticed that equations \eqref{shannon entropy} and \eqref{von Neumann entropy eigenstates} are mostly identical. That changes quickly when we introduce <em>Quantum Mutual Information</em>.</p> <p>To understand Quantum Mutual Information, lets give Alice one qubit and Bob another. We can entangle them such that our input state matches that of equation \eqref{entangled state}. We can then model the mutual information between Alice’s and Bob’s qubits as,</p> <p>\begin{equation} I(A:B) = S(\rho_A) + S(\rho_B) - S(\rho_{AB}) \end{equation}</p> <p>where \(\rho_A = Tr_B (\rho)\) and \(\rho_{AB}\) is the density matrix describing the state of the Alice’s qubit. Tracing over part of the system, in essence ignores that part of the system. When we trace over Bob’s information we are focused solely on the information of Alice. The physical interpretation of mutual information is thus a quantifiable measurement of the entanglement shared between Alice and Bob. For a highly entangled state such as our bell state, we find a value of two which indicates a maximum entanglement between the two qubits. While quantum mutual information is insightful for understanding entanglement between states, these states remain as initial states. We have made a modification to our system between the classical and quantum cases. The modification is the usage of Bob. In our classical noisy channel arrangement, Bob received some data \(Y\) from Alice whose input was \(X\). The classical mutual information was a correlation between input and output. Therefore, We need another tool that is analogous to classical mutual information, that will evaluate how a quantum channel could alter this entanglement. Luckily, we’ll find a familiar analogy to classical mutual information.</p> <p>The flow of quantum information is quantifiable by how much entanglement passes from the initial state to the final state. <em>Quantum Coherent information</em> is a quantitative measurement of this transfer. To understand it better, lets bring in a third qubit belonging to Carla. Let’s suppose Alice and Carla entangle their qubits together and create a Bell Pair like that of eq. \eqref{entangled state}. Bob’s qubit is initially in a \(\ket{0}\) (spin-up) state with \(100\%\) probability. We now define a quantum channel from Alice to Bob where we aim to pass the entanglement. A popular quantum channel that is well known to preserve entanglement is done using a SWAP gate. The SWAP gate will do just as it says; swap the values of Alice’s and Bob’s qubits and with it the information associated. Figure 4 shows this quantum information channel, outlining the SWAP of entanglement from Alice’s qubit to Bob’s.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/info_channel/quantum_circuit-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/info_channel/quantum_circuit-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/info_channel/quantum_circuit-1400.webp"/> <img src="/assets/img/info_channel/quantum_circuit.png" class="img-fluid rounded z-depth-1" width="500" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p class="figcaption"><em>Fig 4: Alice and Carla initially have an entangled bell pair. After the SWAP gate is carried out Carla and Bob are now entangled and Alice is in the \(\ket{0}\) state.</em></p> <p>We can now define our quantum coherent information as,</p> <p>\begin{equation} I_C(\rho_{in,AB},\Xi) = S(\rho_{out,B})-S(\rho_{out,BC}). \end{equation} where, \(\rho_{in,AB}\) is the input to the channel and \(\Xi\) is the channel itself. One should note here, that the Channel \(\Xi\) is equivalent to \(\rho_{out,B}\). The difference in notations is that \(\Xi\) expresses the operators that make up the channel, and their acting on the initial states. Whereas, \(\rho_{out,B}\) is the output state which is a result of this channel. This is because \(\Xi\), mathematically is a mapping from initial states to final states. The analogy to the classical mutual information (Eq. \eqref{Mutual Information}) should be more evident now, and as we did in the classical case, we elevate this to the Quantum Channel Capacity by taking,</p> <p>\begin{equation} Q(\Xi) = \max_{\rho_{in,AB}}I_C(\rho_{in,AB},\Xi). \end{equation}</p> <p>Just as it was in classical information theory, the channel capacity is a measurement of reliability in transferring quantum information (ie. entanglement). So long as entanglement continues to be the best metric of a quantum computer, measuring the propagation of this entanglement, remains to be an important calculation as we further progress quantum computers.</p> <p>We have spoken in detail about two channels of information, yet there remains two more. Channels that change Classical information to Quantum information (This is a process that creates entanglement) and vice versa (Measurement). These channels are handled differently, and subsequently will have to wait until future writing.</p> <p>[1] M. M. Wilde, “Preface to the second edition,” <em>Quantum Information Theory</em>, p. xi–xii</p> <p>[2] L. Gyongyosi, S. Imre, and H. V. Nguyen, “A survey on quantum chan- nel capacities,” <em>IEEE Communications Surveys &amp; Tutorials</em>, vol. 20, no. 2, p. 1149-1205, 2018.</p> <p>[3] Named after Claude Shannon, a mathematician and engineer who is often referred to as the “father of information theory”.</p> <p>[4] We will be using \(\log_2\) for the entirety of this post for consistency.</p>]]></content><author><name></name></author><category term="quantum,"/><category term="informationtheory"/><summary type="html"><![CDATA[by Eric Aspling]]></summary></entry><entry><title type="html">Quantum Ergodic Theorem</title><link href="https://lawlergroup.lassp.cornell.edu/blog/2022/QET/" rel="alternate" type="text/html" title="Quantum Ergodic Theorem"/><published>2022-08-06T16:40:16+00:00</published><updated>2022-08-06T16:40:16+00:00</updated><id>https://lawlergroup.lassp.cornell.edu/blog/2022/QET</id><content type="html" xml:base="https://lawlergroup.lassp.cornell.edu/blog/2022/QET/"><![CDATA[]]></content><author><name></name></author><category term="quantum,"/><category term="thermodynamics,"/><category term="statmech"/><summary type="html"><![CDATA[by Gaurav Gyawali]]></summary></entry><entry><title type="html">Do Quantum Circuits Outperform Neural Networks?</title><link href="https://lawlergroup.lassp.cornell.edu/blog/2021/Do-quantum-circuits-outperform-neural-networks/" rel="alternate" type="text/html" title="Do Quantum Circuits Outperform Neural Networks?"/><published>2021-10-13T16:40:16+00:00</published><updated>2021-10-13T16:40:16+00:00</updated><id>https://lawlergroup.lassp.cornell.edu/blog/2021/Do-quantum-circuits-outperform-neural-networks</id><content type="html" xml:base="https://lawlergroup.lassp.cornell.edu/blog/2021/Do-quantum-circuits-outperform-neural-networks/"><![CDATA[<p>Quantum machine learning <a href="https://www.protocol.com/manuals/quantum-computing/machine-learning-ai-quantum-computing-move-beyond-hype">has received a lot of hype</a>. But there is a simple way to see if the hype is just hype: classical simulation! In an apples-to-apples comparison, can a quantum circuit outperform a classical neural network?</p> <p>Recently, I was reading this very nice article by Taylor Patti (Harvard, Nvidia) and collaborators (<a href="https://arxiv.org/pdf/2106.13304.pdf">see arXiv</a>) and they found a nice application of an idea I had: use pytorch to simulate quantum circuits. They managed to simulate a quantum circuit with 512 qubits that is 13 gates deep and were motivated to do so for an application with time reversal symmetry that allows them to use real numbers instead of complex ones which pytorch is not well setup to handle. The results from their GPU seem impressive and strengthens the motivation for this post: is a quantum circuit good at machine learning and for what applications is it good at? Combinatorial optimization?</p> <p>Of course, machine learning is a broad subject and neural networks are not the only machines around. There are many methods and nobody has created <a href="https://en.wikipedia.org/wiki/The_Master_Algorithm">Pedro Domingo’s “Master algorithm”</a>. It could be that say at image classification or some other common neural network application, a quantum circuit fails miserably compared to neural networks. But if this is the case, it doesn’t mean the quantum circuit is a poor machine in general, just for this one application. So a weaker version of this post’s question is: is there an application where a quantum circuit shines compared to other machine learning algorithms?</p> <p>A quantum circuit requires some background to understand. A quick definition is that a quantum circuit is a non-deterministic map from quantum data to classical bit strings. But for more details see for example <a href="https://qiskit.org/textbook/ch-algorithms/defining-quantum-circuits.html">qiskit’s discussion</a>, <a href="https://en.wikipedia.org/wiki/Quantum_circuit">wikipedia’s discussion</a>, or a good book on quantum computing that does not require a physics background such as <a href="https://michaelnielsen.org/qcqi/QINFO-book-nielsen-and-chuang-toc-and-chapter1-nov00.pdf">Nielsen and Chuang</a>. So there are two properties that make quantum circuits different from neural networks: they act on quantum data (a vector in a tensor product space \(V_d\otimes V_d\otimes\ldots\)) instead of \({\mathbb R}^N\) and produce a distribution of outputs for the same input.</p> <p>Here is some code I wrote to create a simple quantum circuit based machine in pytorch. The weight matrix is an orthogonal matrix generated by a QR decomposition. A measurement is performed on the first qubit and a single bit representing the outcome of the measurement is performed along with the collapsed state after measurement is returned.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">QC</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">N</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">N</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">)])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">qr_correct_weights</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">w_indices</span> <span class="o">=</span> <span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">((</span><span class="nb">chr</span><span class="p">(</span><span class="mi">97</span><span class="o">+</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">)))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">x_indices</span> <span class="o">=</span> <span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">((</span><span class="nb">chr</span><span class="p">(</span><span class="mi">97</span><span class="o">+</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">)))</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x_indices</span><span class="o">+</span><span class="s">','</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">w_indices</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span>
        
        <span class="n">proj0_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x_indices</span><span class="o">+</span><span class="s">','</span><span class="o">+</span><span class="s">'az'</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">]]))</span>
        <span class="n">proj1_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x_indices</span><span class="o">+</span><span class="s">','</span><span class="o">+</span><span class="s">'az'</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">]]))</span>
        <span class="n">norm0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x_indices</span><span class="o">+</span><span class="s">','</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">x_indices</span><span class="p">,</span><span class="n">proj0_x</span><span class="p">,</span><span class="n">proj0_x</span><span class="p">)</span>
        <span class="n">norm1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x_indices</span><span class="o">+</span><span class="s">','</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">x_indices</span><span class="p">,</span><span class="n">proj1_x</span><span class="p">,</span><span class="n">proj1_x</span><span class="p">)</span>
        <span class="n">p0</span> <span class="o">=</span> <span class="n">norm0</span><span class="o">/</span><span class="p">(</span><span class="n">norm0</span><span class="o">+</span><span class="n">norm1</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(())</span> <span class="o">&lt;</span> <span class="n">p0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="s">'0b0'</span><span class="p">,</span><span class="n">proj0_x</span><span class="o">/</span><span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">p0</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="s">'0b1'</span><span class="p">,</span><span class="n">proj1_x</span><span class="o">/</span><span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p0</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">qr_correct_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># We may need to call this every now and then to fix numerical errors (maybe once per epoch?)
</span>        <span class="n">Q</span><span class="p">,</span><span class="n">R</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">qr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">view</span><span class="p">((</span><span class="mi">2</span><span class="o">**</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span><span class="mi">2</span><span class="o">**</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">)))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span><span class="o">@</span><span class="n">torch</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">sign</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">R</span><span class="p">)))).</span><span class="n">view</span><span class="p">([</span><span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">)])</span>
        
    <span class="k">def</span> <span class="nf">grad_symmetrize_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># If gradient is a symmetric matrix, then it will preserve the orthogonality of the weights
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span> <span class="o">+</span> 
                                    <span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">permute</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="o">+</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">)]))</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">grad_symmetrize_</span><span class="p">()</span> <span class="c1"># This line doesn't seem to matter!</span></code></pre></figure> <p>It is a beginning only, but seems to show some promise. I was able to show the weight matrix remained orthogonal after a little training.</p> <p>Now to pose the real test we need a database of quantum data: a collection of vectors in a tensor product space and a problem associated with this data which can be solved either by a quantum circuit or a neural network (by flattening the tensor product space). In a future post, we may post an ideal data set or data sets for this purpose: one composed of high rank tensors found in quantum simulations of quantum materials. However, an alternative in the mean time might be to study 4D data such as 3D videos like <a href="http://www.scan-net.org">ScanNet</a>. Neural networks perform well on one (bar codes?) and two dimensional (images) data sets but they seem to struggle more as you go up in dimension. Perhaps this is an opportunity for machines built out of quantum circuits or inspiried by quantum circuits.</p> <p>I will proprose this problem one day for a student in my group, but if you find this idea interesting, feel please go ahead with your own “experiments” and let us know how it went!</p>]]></content><author><name></name></author><category term="quantum,"/><category term="neuralnets"/><summary type="html"><![CDATA[by Michael Lawler]]></summary></entry><entry><title type="html">The art of quantum</title><link href="https://lawlergroup.lassp.cornell.edu/blog/2020/why-think-quantum/" rel="alternate" type="text/html" title="The art of quantum"/><published>2020-05-29T20:15:21+00:00</published><updated>2020-05-29T20:15:21+00:00</updated><id>https://lawlergroup.lassp.cornell.edu/blog/2020/why-think-quantum</id><content type="html" xml:base="https://lawlergroup.lassp.cornell.edu/blog/2020/why-think-quantum/"><![CDATA[<h3 id="thought-of-the-day">Thought of the day:</h3> <p>A Quantum world is to modern art as our ordinary world is to classic art. In art, the modern form is obtained by changing the rules of the classic form and introduces surprising and unexpected results. Similarly a quantum world is like our ordinary world but with the rules changed. Indeed, if we don’t observe the quantum world, then it seems anything can happen much like modern art.</p> <p>The best known example of this anything-goes-until-you-measure phenomena is the wave function: a particle seems to exist in many places at the same time until you measure its position and find it located at a specific spot–the collapse of the wave function. However, does this extend to time? Can a particle exist at multiple points in time? The answer is yes! But, like the spatial case, it can’t violate the causality of classical physics upon measurement. Check out this interesting paper on the subject by <a href="https://arxiv.org/pdf/2101.09107.pdf">Purves and Short</a> at the university of Bristol.</p>]]></content><author><name></name></author><category term="quantum,"/><category term="thought"/><summary type="html"><![CDATA[by Michael Lawler]]></summary></entry></feed>